# 人工知能と社会メモ　2016.11.09

### 質問票への回答のための資料

*   Wikipedia の正確性
    *   [ブリタニカとWikipedia を比較して、それほど差がないことが分かったのが10年程前。](https://www.cnet.com/news/study-wikipedia-as-accurate-as-britannica/)
    *   調べたのは、雑誌 Nature
    *   実際には、分野、話題により、正確性や話題のカバーの程度は、大きく違う。
    *   また、言語により、大きく違う場合も多い。たとえば、
        *   [IBM Watson](https://en.wikipedia.org/wiki/Watson_(computer)) v.s. [ワトソン](https://ja.wikipedia.org/wiki/ワトソン_(コンピュータ)),
        *   [京都大学](https://ja.wikipedia.org/wiki/%E4%BA%AC%E9%83%BD%E5%A4%A7%E5%AD%A6) v.s. [Universität Kyōto](https://de.wikipedia.org/wiki/Universit%C3%A4t_Ky%C5%8Dto),
        *   [Term logic](https://en.wikipedia.org/wiki/Term_logic) v.s. [Begriffslogik](https://de.wikipedia.org/wiki/Begriffslogik) v.s. 日本語版がない
        *   [Reliability of Wikipedia](https://en.wikipedia.org/wiki/Reliability_of_Wikipedia)　v.s. [维基百科的可靠性](https://zh.wikipedia.org/wiki/维基百科的可靠性) v.s. 日本語版がない　　（[可靠性](http://cjjc.weblio.jp/content/可靠性)）
        *   [第五世代コンピュータ](https://ja.wikipedia.org/wiki/第五世代コンピュータ) v.s. [Fifth generation computer](https://en.wikipedia.org/wiki/Fifth_generation_computer)　
            *   このケースの場合は、英語版の編集の歴史を見ると、さまざまな対立した意見で内容が書きかえれられていることがわかる。
            *   それが研究対象たりえる。
        *   そもそも Wikipedia とは何か？
            *   [その執筆の方法の説明](https://ja.wikipedia.org/wiki/Wikipedia:%E8%A8%98%E4%BA%8B%E3%82%92%E5%9F%B7%E7%AD%86%E3%81%99%E3%82%8B#.E6.A4.9C.E8.A8.BC.E5.8F.AF.E8.83.BD.E3.81.AA.E5.86.85.E5.AE.B9.E3.82.92.E6.9B.B8.E3.81.8F)：「独自の説の発表」という項目に注意！
            *   Wikipedia は、基本的には、まとめサイト。
            *   辞書や本や論文が間違えていれば、それも間違うというのが「正しい Wikipedia のあり方」。

### 前回までの資料　(少し修正）

## AIのタイプ

ディープラーニング、深層学習という言葉を聞くことがあると思う。これは機械学習と呼ばれるAIの一分野の話。ただ、機械学習の専門家は、それを、あまり人工知能と呼ばなかったりするが、これは人工知能学会の重要な分野の一つだから、まぎれもない人工知能だろう。

それから、[Apple の Siri](http://www.apple.com/jp/ios/siri/) 　[＊](https://ja.wikipedia.org/wiki/Siri)　のような、声で質問すると答えてくれるツールに使われている技術は、音声の認識、文の認識、答えの「探索」、回答文の作成、機械による発話、という複数の異なる技術が使われているが、こういうものを全体として[自然言語処理](https://ja.wikipedia.org/wiki/%E8%87%AA%E7%84%B6%E8%A8%80%E8%AA%9E%E5%87%A6%E7%90%86)というが、答えの「探索」の部分は、自然言語処理とは言わない。

ここまででも、機械学習、自然言語処理という二つの異なる技術が出てきた。AIのタイプは、どれくらいあるのだろうか？

もちろん、その答は、人により、まちまちで、この人などは[少なくとも３３ある](http://simplicable.com/new/types-of-artificial-intelligence)としている。

ただ、これはちょっと細かすぎるし、最近話題のAIと関係がないものも多い。

現在、話題になっているAIの種類、あるいはAIの技術の種類は、大雑把に言って、

1.  自然言語処理
2.  機械学習、特にニューラルネットワークによるもの
3.  大規模データからの情報抽出

の三つであるといえる。三番目のものは、[データマイニング](https://ja.wikipedia.org/wiki/%E3%83%87%E3%83%BC%E3%82%BF%E3%83%9E%E3%82%A4%E3%83%8B%E3%83%B3%E3%82%B0)とか[ビッグデータ](https://ja.wikipedia.org/wiki/ビッグデータ)などのキーワードで、マスコミなどで見かけるものだと思えばよい。

先に挙げた、日本で有名な三つのAIの事例は、次のように、これらに対応する。

1.  自然言語処理：　IBM Watson, 東ロボ君
2.  機械学習：　AlphaGo
3.  大規模データからの情報抽出：　IBM　Watson

この内、自然言語処理は、IBM Watson, 東ロボ君においては、インターフェース、特に入力の部分で重要な技術だが、これが、これらのコアの部分というわけではない。この自然言語処理、特に、発話された言葉や書かれた言葉を認識する技術の進歩は、現在のAIブームの基盤というか、必要な前提、あるいは、縁の下の力持ち、とでもいうべき重要な技術であるが、これは、すでに、当然のもの、McCarthy が言った　As soon as it works, no one calls it AI any more.　の、it works のレベルに近づきつつある。つまり、もう、それは人工知能と呼ばれる域を脱しつつある。だから、特に、それにより人工知能が最近注目されているわけではなく、注目が集まっているのは、実は、２の機械学習、特に、デープラーニングのような、人間の脳神経のシミュレーションから始まったニューラルネットワークと呼ばれる技術の進化と、３の大規模データからの情報抽出であるといえる。

*   ちなみに、人工知能の一種ともみなせる自動翻訳は、英語とドイツ語の様な、もともと近い言語の間では、ほぼ実用に近づきつつある。しかし、これは言語処理と呼ばれ、なぜか、あまり、人工知能と呼ばれない。こういうのは、カナ漢字変換が人工知能とみなされなくなったように、「当たり前のもの」「本当の知能と関係ないもの」、McCarthy がいう、 no one calls it AI any more　の it になりつつある。

以上の説明からすると、東ロボ君のAIとしての本体が、最近流行のものではないことになる。実際は、東ロボ君は、ひとつのシステムではなくて、問題文を認識するという所以外は、数学、英語、など、それぞれの科目の問題を解くことに特化されて作られたコンピュータシステムの集まりである。その中で、国語などは人工知能と言って専門家も違和感を持たないだろいうものを使っているらしいが、数学のある種の問題を解くためには、[Quantifier elimination](https://en.wikipedia.org/wiki/Quantifier_elimination)　というアルゴリズムが使われている。株のアルゴリズム取引が、AIによる取引とみなされないことが多いように、人によっては、このアルゴリズムはAIだとみなさないことがあり、AIの専門家には、東ロボ君に批判的あるいは冷淡な人いるのは、このため。しかし、プロジェクトのもともとの目的が、AIというより、コンピュータに仕事が奪われる、という警鐘をならすものだったことからしたら、当然。

この講義では、現在、世界的に流行の２と３のタイプのAIを中心に、しかし、「人から仕事を奪う」可能性を持つという点では、まったく変わらない、東ロボ君のようなシステムも含めて考えていくことにする。

そこで、まず、簡単に、２の機械学習、特にニューラルネットワークによる学習と、３の大規模データからの情報抽出の大雑把な説明をしておく。

## 機械学習

まず、Wikipediaを見てみる。実は、IT分野の話題では、Wikipedia の情報の信用度は非常に高い。一方で、IT分野の書籍、すくなくとも日本語の書籍のレベルは低いので、たとえば、英語の Wikipedia と、日本語のIT関係本を比較すると、英語の Wikipedia の方が、はるかにレベルが高く信頼性が高いことが多い。これは、英語という巨大なユーザ集団をもつ言語で書かれた Wikipedia の文章が、世界中のIT関係者により「監視」されている一方で、日本語のIT関係本、特に非専門家向けのものが、かならずしもITの専門家ではないライターたちにより書かれる、また、書かれたら容易に直せない、ということを考えれば、当然のこと。

*   実は、IT関係の新聞記事が、ほとんど嘘ばかりというのは、専門家の間では有名なこと。これは日本の記者たちが文系の人で、ITを全く理解できていなかったため。ただし、世代交代が起きたためか、最近は改善されている。
*   新聞に書いてあることだからと信用してはいけない。技術関係のことでいえば、技術者たち、理系の人たちが書いている Wikipedia の信頼性の方がはるかに高い。

ということで、話を本題にもどして、機械学習の Wikipedia 記事。

*   [日本語](https://ja.wikipedia.org/wiki/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92)
*   [英語](https://en.wikipedia.org/wiki/Machine_learning)

読んでもピンとこないかもしれないので、実例で説明。

### Googleとキュウリ農家

これは夏に、Google 日本支社を訪問してGoogle のAIへの取り組みを取材した際に、担当者が特に強調していた事例。

元技術者のキュウリ農家の小池誠さんが、Google の機械学習用のツールを利用して、キュウリの自動仕分け機械を作ったという話。

[これは、Google の機械学習を広報するサイトで読める。](http://googlecloudplatform-japan.blogspot.jp/2016/08/tensorflow_5.html)

これの中ほどにある、２LからCまで、さまざまな等級に分けられたキュウリの写真があるが、これが機械学習。

分類は小池さんのお母さんのスキルで、写真を等級に分けて（それはお母さんが行う）、それを Google の TensorFlow というもので学習させた。

そうすると、その等級の意味を、TensorFlow　が学習し、自分で、分類ができるようになったということ。

TensorFlow を新人の手伝いと考えれば、その意味がわかるだろう。

こういうのが機械学習。

### ここから今回の資料

前回、機械学習というものを、その例である、キュウリの等級分けを例に見た。

[機械学習](https://ja.wikipedia.org/wiki/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92)には、非常に多くの異なる方法がある：　Wikipedia [英語](https://en.wikipedia.org/wiki/Machine_learning#Approaches)　[日本語](https://ja.wikipedia.org/wiki/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92#.E6.8A.80.E6.B3.95)

このキュウリの投球分けに使われていたのがディープラーニングという方法で、機械学習の内、最近もっとも注目されているもの。

これの仕組みを説明するには数学が必要で難しくなるので、入門である今はさける。（後で、もしかしたらやる。）

ポイントは、[ここ](http://matome.naver.jp/odai/2140635573608360401)で説明されている「従来職人技だった特徴量抽出を教師なし学習でやってしまうという点で画期的です。これにより、特徴量の設計が難しいがゆえに機械学習でうまく扱えなかった問題を改善できるかもしれません。」という点。

つまり、キュウリのある等級は、これくらいの大きさで、これくらいの曲がり具合で、…、などといういくつかのキュウリの特徴で決まっているはずなのだが、その特徴を、人間でなくて、機械が勝手に学習してくれるということ。

たとえば、[上のページ](http://matome.naver.jp/odai/2140635573608360401)に「ニューロンの一つが「猫」を認識」と書いてあるが、その意味は、[http://d.hatena.ne.jp/Zellij/20130608/p1](http://d.hatena.ne.jp/Zellij/20130608/p1)に解説してあるように、機械が自分で「これらは似ている」という画像の分類法（特徴量）を多数作り出したが、そのうちの一つが「猫の画像」と、ほぼ一致していたということ。

この「猫の認識」の実験はグーグルでおこなれた実験で、報道ではよく、

![20130608212518](http://cdn-ak.f.st-hatena.com/images/fotolife/Z/Zellij/20130608/20130608212518.png)

という画像とともに説明されていて、あたかもグーグルのコンピュータが、たくさんの画像を一人で見て、その後、「これが猫です」と言って、この画像をプリントしたかのように説明されることがあるが、そうではなくて、グーグルのコンピュータが無数に作り出した、画像の特徴量（画像判別方法）の一つに、猫の画像に非常によく反応するものがあった、ということ。また、この画像は、その特徴量を一般の人にわかりやすくするために、その特徴量から人間が作り出したもの。

機械が作り出した一つの特徴量が「猫の画像の特徴量といってよい」という判断や画像の作成は人間がやっていることに注意。

これに反して、最近、碁の一流プレイヤーを破った [AlphaGo](https://ja.wikipedia.org/wiki/AlphaGo) は、最初は人間から学んだが、[そののち、自分自身と対戦して強くなった](https://en.wikipedia.org/wiki/AlphaGo)。これとキュウリの等級分け、猫の画像の認識、の違いに、AIと未来の労働、つまり、どんな職が機械に奪われるのかのヒントが含まれているので、後で再論。

### ディープラーニングは本当に画期的か？

現在の報道の多くは、「ディープラーニングというものができて、それを使うとAIが現実になる」というように読める論調のものが多い。

つまり、ディープラーニングという技術革命が起きて、それにより、ディープラーニング前とディープラーニング後で、世界がガラリと変わりつつある、という風に理解されているのではないかと思える報道が多い。

実は、ディープラーニングブームが巻き起こる前に、すでに機械学習は、いろいろな場面で実用化されている。

そして、そういう、さまざまな努力の総体により、AIが大きな社会的影響を引き起こしそうな状況が生まれている。

だから、あまりに、ディープラーニングにスポットライトをあてると、人工知能の社会影響をかんがえるときには、間違いのもととなる。

たとえば、次に、ディープラーニングとともに、今日のAIブームの火付け役になったと思われる、IBMの人工知能、アメリカの人気クイズ番組で、歴代チャンピオンに買った Watson の話をするが、Waton は、ディープラーニングと関係がない。

また、先に説明したように、IBMは、Watson のようなものをAIとさえ呼ばない。

日本では Watson とディープラーニングと人工知能が、ほぼ同時に話題になったように見えるが、実際には、1990年代から人工知能技術は現実的で、また、WWWの巨大データを処理するために必要な技術になっており、また、アメリカでは、ディープラーニング騒ぎの前に Watson 騒ぎがあった。Watson が、クイズ番組 [Jeoperdy!](https://ja.wikipedia.org/wiki/%E3%82%B8%E3%82%A7%E3%83%91%E3%83%87%E3%82%A3!) でチャンピオンになったのが2011年。これに対し、[デープラーニングが評判になり始めたのは2012年](http://business.nikkeibp.co.jp/article/bigdata/20150419/280107/?ST=print)。しかも、先に説明したように、この二つには直接の関係がない。

そこで、Watsonの話をする前に、どの様な経緯で、現在の様なAIブームが起きたのか、その歴史を簡単に見ておく。

## AIの歴史

今のAIブームは、三回目のブームだと言われることが多い。最初のブームは、1950年代、AIというものが生まれたとき、第2次ブームは1980年代だということで大抵の見方は一致している。

このブームというのは、例えば、アメリカ、日本、ヨーロッパなどで、政府や産業が、大量の研究費・開発費をAIに落とした時期のことを言う。そして、ブームとブームの間には、「冬の季節」があり、それは、AIに投入される資金が大幅に減らされた時期を意味する。

ところが、第二次ブームとは何だったのか、というのは、説明がいろいろある。現在のブームが Watson的なものと、ディープラーニング的なもののごった煮であるように、この時代の主役は一つではなく、次の三つのものの混交だった：

1.  [日本の通産省の新世代コンピュータ（第5世代コンピュータ）プロジェクト](https://ja.wikipedia.org/wiki/第五世代コンピュータ)
2.  [エキスパート・システム](https://ja.wikipedia.org/wiki/エキスパートシステム)
3.  [コネクショニズムの再評価](https://ja.wikipedia.org/wiki/%E3%82%B3%E3%83%8D%E3%82%AF%E3%82%B7%E3%83%A7%E3%83%8B%E3%82%BA%E3%83%A0)

ただし、混交とかごった煮というのは、この三つが融合していたというのでなくて、それぞれ、別のもの、ある意味ではライバルとして存在しており、それらの全体で「（第2次）人工知能ブーム」だったという意味。

そして、これの１，２が、Watson に、３がディープラーニングに対応する。

そのことを念頭において、いくつかの「歴史」を見てみる。

日本語

*   [日本人工知能学会による歴史](https://www.ai-gakkai.or.jp/whatsai/AIhistory.html)
*   [ITソリューション塾](http://blogs.itmedia.co.jp/itsolutionjuku/2015/07/post_105.html)
*   [weblio](http://www.weblio.jp/content/人工知能の歴史) [weblio とは](https://ja.wikipedia.org/wiki/Weblio)
*   [Wikipedia](https://ja.wikipedia.org/wiki/人工知能の歴史)

英語

*   [Wikipedia](https://ja.wikipedia.org/wiki/人工知能の歴史)
*   [AI Topics](http://aitopics.org/misc/brief-history) [AAAI](https://ja.wikipedia.org/wiki/%E3%82%A2%E3%83%A1%E3%83%AA%E3%82%AB%E4%BA%BA%E5%B7%A5%E7%9F%A5%E8%83%BD%E5%AD%A6%E4%BC%9A) のサイト　AAAIは世界最大のAIの学会。その意味では一番信頼がおける歴史かもしれない。林も、1980年代にAAAIに一度だけ参加したことがある。すごく規模が大きく、また、数十名の報道関係者がうろうろしていて驚いた。

以上の様に、人によりAIの歴史の見方は、異なり、特に、なぜ、第3次AIブームが起きたか、これは本物か、ということの説明は、人により大きく異なる。

これには

*   人工知能研究とは、人間の知能を人工的に作り出しことである。
*   そして、それにより、知能とは何かを知ることである。

という立場と、

*   人工知能とは、人間の知的労働を肩代わりしたり、補助したりするものである。
*   それは機能すればよいのであって、自動車と動物の移動方法が全く違うように、人間の知能とは関係がなくてもよい。

という立場の様に、AIに対する異なる立場があること。

それから、人工知能で実現されるものが、大雑把に：

1.  人間の論理的・合理的判断能力…考える能力、推論力
2.  人間の視覚、聴覚などの感覚による認識能力…直感力

の二つに分類できることにある。

ディープラーニングは、主に２であり、Watson は、１である。

*   ただし、AlphaGo のように、ディープラーニングは１にも応用されるようになってきている。
*   しかし、それは性能を上げるために使われており、ディープラーニングの様なものだけで１を行うことは不可能。
*   ディープラーニングなどの学習アルゴリズムは、本質的に間違えることがある。その間違いを正していくことが「学習」だからである。
*   一方、１の方は、データの誤解釈などで間違える、データが間違えている、などで、間違うこともあるが、基本的には、「論理」の能力なので「間違えない」のが基本。

こう考えると

*   第2次AIブームは、主に「人間の論理的・合理的判断能力…考える能力、推論力」を行うAIが脚光を浴び、
*   現代のAIブームは、2016年の今では、あえていえば「人間の視覚、聴覚などの感覚による認識能力…直感力」を行うAIが脚光を浴びている

といえる。

しかし、AIが職を奪う、というような問題を考えるとき、より重要なのは、前者である。

ただし、先に述べたように、それを実現する際に、AlphaGo の様に、ディープラーニングのような「人間の直観力を代替する認識装置」が使われるようになってきている、といえる。

つまり、人間から職を奪う、特に頭脳労働を行う職、たとえば、弁護士、会計士、医師、教師、ホワイトカラーなどの職を奪う可能性があるのは、AlphaGo 的なもので補強された Watson 風のシステム、だろうと考えられる。

そこで、その Watson の話。

## IBM Watson

IBM では、昔からAIの研究が続けられており、大きな成果を上げてきた。

たとえば、1996, 1997年に、当時のチェスの世界チャンピオン（グランドマスター）[ガリル・カスパロフ](https://ja.wikipedia.org/wiki/%E3%82%AC%E3%83%AB%E3%83%AA%E3%83%BB%E3%82%AB%E3%82%B9%E3%83%91%E3%83%AD%E3%83%95)を破ったスーパーコンピュータ [Deep Blue](https://ja.wikipedia.org/wiki/%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%BB%E3%83%96%E3%83%AB%E3%83%BC_(%E3%82%B3%E3%83%B3%E3%83%94%E3%83%A5%E3%83%BC%E3%82%BF)#.E6.A6.82.E8.A6.81) は、その一つ。

*   上のカスパロフのWIKIPEDIA記事に、「西欧では[チェス](https://ja.wikipedia.org/wiki/%E3%83%81%E3%82%A7%E3%82%B9 "チェス")は理知的なゲームの代表、「人間の知性の極み」と見なされており[[1]](https://ja.wikipedia.org/wiki/%E3%82%AC%E3%83%AB%E3%83%AA%E3%83%BB%E3%82%AB%E3%82%B9%E3%83%91%E3%83%AD%E3%83%95#cite_note-kettei-1)、チェス人口が多いので、カスパロフは頭の良い人物の代表として、しばしば言及される人物でもある。」というフレーズがあることに注意。
*   また、20年後の現在では、チェスより複雑な将棋の世界で、[三浦弘行九段が対局中に将棋ソフトを使用したのではないかと疑われている](http://mainichi.jp/articles/20161026/k00/00m/040/081000c)ことにも注意。

そして、この Deep Blue のプロジェクトの後継プロジェクトが、アメリカのクイズTV番組で、一番といってもよい人気を誇る**[Jeopardy!](https://ja.wikipedia.org/wiki/%E3%82%B8%E3%82%A7%E3%83%91%E3%83%87%E3%82%A3!)**でチャンピオンに勝つことを目的とした、[IBM　Watson](https://ja.wikipedia.org/wiki/ワトソン_(コンピュータ)) のプロジェクトだった。

まず、Jeorpady! がどんなクイズかをみる。[https://www.youtube.com/watch?v=pFhSKPOF_lI](https://www.youtube.com/watch?v=pFhSKPOF_lI)

[Wikipedia の解説](https://ja.wikipedia.org/wiki/ジェパディ!)：　出題と解答の文の形が通常のクイズと反対になっている（肯定文で出題し、疑問文で答える）のを除けば、通常の「知識量を競う」クイズ。

[そして、WatsonのJeorpady!](https://www.youtube.com/watch?v=KVM6KKRa12g)

Jeorpady! において示された Watson の機能とは「英語で質問すると、それに答える」ことであった。

*   注. 現在の Watson の機能は、この機能だけではない。

この機能は、第2次ブームの際の、英語や日本で質問できるというエキスパートシステムと同じ。

それは、次のことからわかる。

第2次ブームで、エキスパート・システムの可能を最初に示したのが、スタンフォード大学の Mycin というシステムだった。

[Mycin](https://ja.wikipedia.org/wiki/Mycin) : 患者の症状から、原因の病原菌・ウィルスを推測し、治療に適当な抗生物質を示唆するシステム。

[白血病を診断した Watson](http://healthpress.jp/2016/08/ai-10ibm-watson_2.html)

この二つが酷似していることがわかる。

ただし、Watson と、Mycin には、いくつかの大きな違いがある。

### 違い１　質問方法

Mycin では、Wikipedia の記事にあるように、症状の入力には、 Yes/No などの選択肢で行われた。つまり、Web でアンケートに答えるような感じ。

Watson では、、これが自然言語（英語や日本語）で行える。

自然言語を機械が、かなり「理解」できる様になったということが、第2次ブームの時代と、現在の非常に大きな違いといえる。

ただし、「理解」というように、理解に括弧「」がついていることに注意してほしい。

### Watson の「理解」と人間の理解

これは、機械が、我々人間が行う「文章を読んで、その内容・意味を理解し、文章から情報を得る」というような作業と同じことができるようになったということではない。

Jeorpady!の様なクイズのチャンピオンは、クイズでは、ほとんどの大学教授より、良い成績を残せるはずだが、それだからと言ってクイズのチャンピオンは、大学教授にはなれない。高校の先生にさえなれないはず。

たとえば、「大政奉還をした徳川将軍は？」というような、ある明白な歴史イベントの発生年を聞くような問には、クイズのチャンピオンや Watson は難なく答えるだろうが、

「300年も続いた徳川幕府が大政奉還により、その権力を放棄することになった理由を述べよ」とか「大政奉還における徳川幕府と朝廷の関係を説明せよ」という問題には、現在の Watson でも答える能力は、まだないはず。

「現在の」と書いたのは、Joeperdy! に勝利した後、Watson は、さまざまに改善、拡張され、商品となっているから。

その改善には、、

また、クイズに問われる様な「雑学的知識」では、大学の歴史学の教授をしのぐ知識を持つであろうクイズ・チャンピオンは、

これらの問題に答えることができるはずだが、その答えは、近世日本史の教授の答えには及ばないだろう。

また、Watson に、数学の問題を聞いたら、非常に単純な問題でも、<span class="red">多分</span>、答えられない。（多分と書いたのは、簡単な問題の場合、特に、センター入試風の問題では、これができるようにするのは、技術的には、それほど難しくはない筈だから。）

知力には、さまざま種類がある。

Watson や、エキスパート・システムの持つ知力というのは、基本的には、これらのシステムがデータベースの中に持つ情報を組み合わせて、推論を行うことである。

要するには、推論マシン、[推論エンジン](https://ja.wikipedia.org/wiki/%E6%8E%A8%E8%AB%96%E3%82%A8%E3%83%B3%E3%82%B8%E3%83%B3)である。Mycin の推論エンジンは、実にシンプルなものであって、基本的には、いわゆる[三段論法](https://ja.wikipedia.org/wiki/%E4%B8%89%E6%AE%B5%E8%AB%96%E6%B3%95)の一番代表的なケースである

AならばB

BならばC

よって

AならばC

を自動的にやっているのだと思えばよい。参考： [Wikipedia Inference Engine （推論エンジン）の Architecture](https://en.wikipedia.org/wiki/Inference_engine#Architecture).

三段論法のようなものを形式論理学というが、そこにおける形式的推論は、意味を理解しないでもできる機械的なもの。

だから、それを使って機械であるコンピュータに推論をさせようということになっのがエキスパート・システム。

しかし、人間は、多くの場合、形式的推論の前に、あるいは、他に、直観とか経験による判断を使う。

Deep Learning などのニューラルネットワークによるAIでは、それに類するものができるのだが、エキスパートシステムにはそれがなかった。

また、Watson では、それを取り込もうとしている兆候が見えるのだが、おそらくは、まだ中心的役割は果たしていない。

### 違い２　巨大なデータとその獲得方法の存在

第3次ブーム以前から、AIを研究しているベテランのAI研究者の間では、第3次ブームと第2次ブームのAIには、

差がなく、特に新しい技術が生まれたのではなくて、単に「ビッグデータ」あるいは巨大データが得られるようになっただけだ、というシニカルな見方をする人が結構多い。

たとえば、Watson の開発に従事した日本人として有名な、[日本IBMの武田浩一さんも、同じような意見](https://enterprisezine.jp/iti/detail/6436)を言っている。

*   ただし、これは第3次ブームにシニカルなのではなくて、巨大なデータの存在が本質的だったのだという意見。

これは、AIを技術面だけからみれば、一理ある意見なのだが、AIと社会との関係を考えるときには、「巨大データ」の存在と、

それを活用できるようにした、自然言語のテキストから情報を得る技術が開発されたことが大きい。

そして、もうひとつ、エキスパートシステムでは、存在はしたが、まだ、弱かった「統計的な思考法」とでもいうべき、

「少数のエキスパートが持つ厳密だけが役に立つのではなくて、むしろ、曖昧さを含む大量の社会的知識の方が、むしろ大きな力になる」

という、WEBの時代が生み出した、知識に関するパラダイム・チェンジが大きい。

はっきり語られることはないのだが、その情報の収集方法や推論方式などを見ると、Watson は、明らかに、この哲学をもとに作られている。

そして、そのパラダイム・チェンジとは、以前、話題になった集合的知識というもの。

ということで、次に、これを説明。

### 集合的知識 Collective Knowledge

[集合的知識](https://ja.wikipedia.org/wiki/集団的知性)とは？（この Wikipedia 記事では、集合的知性になっているが、知識の方がよい！）

以下では、集合知と書く。

[英語版 Wikipedia では。](https://en.wikipedia.org/wiki/Collective_intelligence)

簡単に説明すると、

*   集合知は、ある集団が、集団としてもつ有機的に結合された知識という意味で言われることが多いが、ここでは、一人のひとの知識、あるいは、非常に少数の人たち共通して持つ知識（たとえば、専門家の知識）と異なり、不正確な知識を持つ人たちも含む、大きな人間集団の知識の総体としてえられる知識のことだと思うことにする。
    *   例えば、選挙結果。
    *   Wikipediaの知識。
    *   Google の検索の Page Rank や Facebook のいいね、Amazon の review、Twitter のフォロワーなど。
    *   Linux などのソフトウェアの[オープン・ソース開発](https://ja.wikipedia.org/wiki/%E3%82%AA%E3%83%BC%E3%83%97%E3%83%B3%E3%82%BD%E3%83%BC%E3%82%B9%E3%82%BD%E3%83%95%E3%83%88%E3%82%A6%E3%82%A7%E3%82%A2)。
    *   [Iowa Electronic Markets](https://en.wikipedia.org/wiki/Iowa_Electronic_Markets)
*   我々は、知識の正確性というのは、統計的なものではなく、正確な知識と推論で得られると思いがちである。
*   この信念が、専門家に大きな価値を与えている。
*   しかし、コンピュータの登場の後、多数の意見をリアルタイムで統計的に処理できるようになってから、特にWEBの登場後、「烏合の衆」と考えられ勝ちな素人の集団が、集団の要素の数が十分多い場合、専門家なみ、場合によっては、専門家より正確な知識を持つことが経験的に知られるようになった。
*   たとえば、Google 検索で、キーワードに関係が深いと思われるページを探すのには、Page Rank という指数が使われている（この Page は、頁の意味ではなくて、Google の創業者 L. Page の名前から来ている）。
*   この説明にあるように、たくさんのリンクがあることを、そのページが肯定的に注目されていることとみなして、注目度が高いもののランクを上にする。
*   現在では、それだけでなく、さまざまな方法が使われていて、Google 検索が、なぜ、的確な答えを出してくるか、その仕組みは管理している技術者でさえ説明できないような複雑なものなっているといわれている。
*   しかし、今でも、検索の最大の要素は、この Page Rank であると言われている。そして、Google が大企業に育った、理由の非常に大きな理由が、この Page Rank により、他の検索エンジンに比較して、はるかに適切な検索結果をユーザに提示できたこと。つまり、ユーザが期待するような結果を出力することが、他の検索エンジンより大幅に多かった。
*   考えてみると、これは不思議なこと。リンクには、例えば、否定的なものもあるだろう。あるWEBページにリンクして、「このページの内容は、嘘だらけだ！」と書くこともできる。
*   しかし、リンクする人とリンクの数が十分に大きい場合、この様な例は、例外として、肯定的なリンクの数に打ち消されてしまい、比較的正確な情報が得られるというのが、経験的に知られた「現実」だった。
*   この様なものが「集合知」の力。つまり、統計的な真理、多くの文書や、多くの筆者が語るものは、正しい可能性が高いということ。
*   ちなみに、質問票への回答の中で、例として挙げた[Reliability of Wikipedia](https://en.wikipedia.org/wiki/Reliability_of_Wikipedia)には、Wikipedia における集合知の問題点も論じられているように、集合知は絶対的なものではない。
*   しかし、おそらくは、これが最大のポイントなのだが、実は、個々の辞書、辞典、専門書や専門家の意見にも、多くの間違いがある。一般に人は気づかないし、出版社は、専門家が注意しても、重要な項目でなければ、分かっていても無視したりする（実際に、岩波広辞苑で林は経験した）。
*   もちろん、謙虚に、自分の書いたもの、自分の知識を振り返れば、そこには多くの間違いが見つかる。
*   これらのことを認識すれば、集合知の精度が思ったより高いということは、不思議なことではなくなる。

## IBM Watson と1980年代のエキスパート・システムの違い（再論）

さて、以上の集合知の話をもとに、IBM Watson と、1980年代のエキスパート・システムの違いを考えてみる。

上で、すでに引用したWatson project に参加された[日本IBMの武田浩一さんのインタビュー記事](https://enterprisezine.jp/iti/detail/6436)の内容を少し引用。

**太字は林の解説。**

*   **1980年代の第5世代コンピュータなどは、**IBMが現在推し進めているコグニティブ・コンピューティングとは何が違うのか？日本IBM 東京基礎研究所 技術理事の武田浩一氏に話を訊いた。
    *   **コグニティブ・コンピューティングとは、IBMが Watson などの同社の「AI」を呼びときの名称。**
    *   **IBMは意識的に、人工知能という名称を避けている。**
*   「（非手続き型プログラミング言語の）Prolog中心の推論エンジンを作ろうというものでした」と言うのは、日本IBM 東京基礎研究所 技術理事の武田浩一氏だ。この第5世代コンピュータは、現在IBMが強力に推し進めているコグニティブ・コンピューティングと考え方は似ている。
*   とはいえ、今と大きく違うのは当時はデータがなかったことだ。もちろん、Webも存在しなかければ、電子化されたWikipediaのようなものもない。「<span class="red">データがなかったことは、ものすごいハンディだったと思います」と武田氏</span>。
*   2011年、米国クイズ番組「ジョパディ!（Jeopardy!）」で人間のクイズ王に勝利したIBMのWatson。クイズの問いに対する答えを導き出すために、今ではWikipediaなどの電子化されたさまざまな情報がある。なので、クイズのような広範な質問に対し答えを見つけるための情報源に困ることはない。逆に言えば、そういった電子化されたあるいは電子化できる情報があるからこそ、Watsonのような自然言語で考えるコンピュータが実現できたとも言える。
*   たとえば、翻訳をしたりクイズに答えたりという場合には曖昧性が重要となります。より正解らしいものを提示する。そのための候補は、1つとは限りません」（武田氏）
*   ありそうな答えを出すことが、当時のAIの推論エンジンではできなかった。今のコグニティブ・コンピューティングでは、統計的な考え方なども取り込んで「たくさん正解が出るように集合的な最適化をしています」とのことだ。
    *   **ここは、第5世代コンピュータについては正しいが、エキスパート・システムについては、少し不正確な説明。**
    *   **エキスパート・システムは、答えが一つでも、それに確実度を表す数字がついていた。**
    *   **だから、単純に真か偽かで割り切ったのではなかった。**
    *   **これが現代の確実でない知識を表す技法の芽となったという意見もある。**
*   **記事では、この間に、IBMのチェスシステム、Deep　Blue で統計的方法が取り入れられ、第5世代コンピュータのころには、希薄だった、統計的で不確実な計算の有用性が認識されたという説明がある。これは実は、上に説明した集合知革命と、深い根において関係があるのだが、それは、今回は省略。かなり哲学的ともいえるものなので。ただ、根本的なパラダイム・チェンジなので、すごく重要なこと。**
*   Watsonの世界は、言い換えればコンピュータに多様性を取り入れるようなものだ。質問の答えは唯一ではなく、少量多品種を容認する世界でもある。そのための情報のインプットをプログラマーが考え手作業でいちちやることは現実的ではない。そこで注目を集めているのが、機械学習というわけだ。
*   「<span class="red">Watsonはデータがあったからうまくいった部分はあるでしょう。</span>データが増えていけば、精度は上がっていきます。今は、データが増えすぎて処理が頭打ちになることはありません。どんどん成長しており、もちろんそれに合わせハードウェアの成長もさらに進むことになる。まだまだ要求は衰えることはないでしょう」（武田氏）

林の、この講義は、経済産業研究所の人工知能の社会影響の研究プロジェクトを背景にしていることは、すでに説明した。

実は、その一環で、武田さんに、林のチームがインタビューを行った。林は病気で参加できなかったが、その記録を聴くと、最初、辞典からのデータ収集に期待をかけたが、実は、役立ったのは、Wikipedia だったという発言がある。

これが、おそらく、Watson　が成功するための条件であった<span class="red">データの存在</span>のことだったと思われる。つまり、上の記事の、2か所の赤文字の発言。

### テキストマイニングと unstructued data

Joeperdy! は雑学知識を問うクイズ番組なので、それに勝つためには、雑学データが必要。

そして、Joeperdy! で必要とさえる雑学データとは、たとえば、統計情報のようなものではない。

それは、テキスト・マイニングというもので、得られるデータで、その生データを用意したのが、先に説明した集合知革命の一つとして生まれた、Wikipedia という、巨大な、unstructued data　base 非構造的データベースであった。

そして、武田さんが言われているように、それが Watson 成功の一つのカギだったが、実は、もう一つのカギが必要だった。それは、上の記事の引用では、省略した、次の様な部分：

*   新たに注目を集めた自然言語処理の世界 　
    *   AIの空白期間を超え、2000年以降になり新たに注目され始めたのが人間が発する質問に対し答えるような、自然言語処理と呼ばれる世界だった。これは、IBMであれば後のWatsonの世界へとつながる。

次回、このテキストマイニングと unstructured data と自然言語処理の話。

簡単に言えば、stuructured data とは、Kulineの出力の様に、データが、その意味において、所定の場所に置かれているようなもの。たとえば、1行目の一番目の欄が本の番号で、3番目の欄が所蔵館などとわかる。

![](aSetOfBooks.PNG)

そして、同じ 0001 の蔵書の unstructured data とは、

No. 0001 の蔵書は、数理研究図書館に所蔵されていて、その配置場所は第一書庫、そして、その請求番号と資料IDは、それぞれ、HAY||10||1 と8788629213 である。

という文章のような data。
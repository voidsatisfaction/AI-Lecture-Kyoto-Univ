# 人工知能と社会メモ　2016.11.16 <font size="+1">(ver.2016.11.20)</font>

昨日、京大の東京オフィスで、「[京大の知](http://www.kyoto-u.ac.jp/ja/tokyo-office/event/kyoudainochi.html/view)」という講演会シリーズの為に、一般向けのAIと社会の話をしてきました。その内容は、この講義で、詳しく、学術的に話す予定のものですが、予め見ておくとよいかもしれません。こちらが[資料](http://www.shayashi.jp/20161115tokyoVer20161116.pdf)です。

### 質問票への回答のための資料

*   [トヨタ　と Connected car](http://ascii.jp/elem/000/001/012/1012038/)

### 前回までの資料　(少し修正）

## AIのタイプ

＜中略＞：　AIのタイプの話で、現在のAIブームの要因の一つである deep learinig などの機械学習の話をし、その後、AIの歴史を見て、一つ前の１９８０年代のAIブームでも、この deep learning と同じ種類のコネクショにズムの話があったこと、しかし、中心はエキスパートシステムや、日本の第５世代コンピュータなどの「論理推論」をする機械であったことを見ました。そして、現在のAIブームでも、これと同じ系統のものとして、IBM Watson があり、しかし、その間に集合知という、大きな革命が起きていることを説明しました。そして、この知識をもとに、１９８０年代には十分な成果を得ることができなかったエキスパート・システムなどの、「合理的な問題を質問をすると、合理的に答えてくれるシステム」が、なぜ、今の時代に成功しつつあるか、その理由を、IBM Watson を例にして説明している、その途中でした。(前回は、[ここ](#2016.11.09)まで。）

## IBM Watson

IBM では、昔からAIの研究が続けられており、大きな成果を上げてきた。

たとえば、1996, 1997年に、当時のチェスの世界チャンピオン（グランドマスター）[ガリル・カスパロフ](https://ja.wikipedia.org/wiki/%E3%82%AC%E3%83%AB%E3%83%AA%E3%83%BB%E3%82%AB%E3%82%B9%E3%83%91%E3%83%AD%E3%83%95)を破ったスーパーコンピュータ [Deep Blue](https://ja.wikipedia.org/wiki/%E3%83%87%E3%82%A3%E3%83%BC%E3%83%97%E3%83%BB%E3%83%96%E3%83%AB%E3%83%BC_(%E3%82%B3%E3%83%B3%E3%83%94%E3%83%A5%E3%83%BC%E3%82%BF)#.E6.A6.82.E8.A6.81) は、その一つ。

*   上のカスパロフのWIKIPEDIA記事に、「西欧では[チェス](https://ja.wikipedia.org/wiki/%E3%83%81%E3%82%A7%E3%82%B9 "チェス")は理知的なゲームの代表、「人間の知性の極み」と見なされており[[1]](https://ja.wikipedia.org/wiki/%E3%82%AC%E3%83%AB%E3%83%AA%E3%83%BB%E3%82%AB%E3%82%B9%E3%83%91%E3%83%AD%E3%83%95#cite_note-kettei-1)、チェス人口が多いので、カスパロフは頭の良い人物の代表として、しばしば言及される人物でもある。」というフレーズがあることに注意。
*   また、20年後の現在では、チェスより複雑な将棋の世界で、[三浦弘行九段が対局中に将棋ソフトを使用したのではないかと疑われている](http://mainichi.jp/articles/20161026/k00/00m/040/081000c)ことにも注意。

そして、この Deep Blue のプロジェクトの後継プロジェクトが、アメリカのクイズTV番組で、一番といってもよい人気を誇る**[Jeopardy!](https://ja.wikipedia.org/wiki/%E3%82%B8%E3%82%A7%E3%83%91%E3%83%87%E3%82%A3!)**でチャンピオンに勝つことを目的とした、[IBM　Watson](https://ja.wikipedia.org/wiki/ワトソン_(コンピュータ)) のプロジェクトだった。

まず、Jeorpady! がどんなクイズかをみる。[https://www.youtube.com/watch?v=pFhSKPOF_lI](https://www.youtube.com/watch?v=pFhSKPOF_lI)

[Wikipedia の解説](https://ja.wikipedia.org/wiki/ジェパディ!)：　出題と解答の文の形が通常のクイズと反対になっている（肯定文で出題し、疑問文で答える）のを除けば、通常の「知識量を競う」クイズ。

[そして、WatsonのJeorpady!](https://www.youtube.com/watch?v=KVM6KKRa12g)

Jeorpady! において示された Watson の機能とは「英語で質問すると、それに答える」ことであった。

*   注. 現在の Watson の機能は、この機能だけではない。

この機能は、第2次ブームの際の、英語や日本で質問できるというエキスパートシステムと同じ。

それは、次のことからわかる。

第2次ブームで、エキスパート・システムの可能を最初に示したのが、スタンフォード大学の Mycin というシステムだった。

[Mycin](https://ja.wikipedia.org/wiki/Mycin) : 患者の症状から、原因の病原菌・ウィルスを推測し、治療に適当な抗生物質を示唆するシステム。

[白血病を診断した Watson](http://healthpress.jp/2016/08/ai-10ibm-watson_2.html)

この二つが酷似していることがわかる。

ただし、Watson と、Mycin には、いくつかの大きな違いがある。

### 違い１　質問方法

Mycin では、Wikipedia の記事にあるように、症状の入力には、 Yes/No などの選択肢で行われた。つまり、Web でアンケートに答えるような感じ。

Watson では、これが自然言語（英語や日本語）で行える。

自然言語を機械が、かなり「理解」できる様になったということが、第2次ブームの時代と、現在の非常に大きな違いといえる。

ただし、「理解」というように、理解に括弧「」がついていることに注意してほしい。

### Watson の「理解」と人間の理解

これは、機械が、我々人間が行う「文章を読んで、その内容・意味を理解し、文章から情報を得る」というような作業と同じことができるようになったということではない。

Jeorpady!の様なクイズのチャンピオンは、クイズでは、ほとんどの大学教授より、良い成績を残せるはずだが、それだからと言ってクイズのチャンピオンは、大学教授にはなれない。高校の先生にさえなれないはず。

たとえば、「大政奉還をした徳川将軍は？」というような、ある明白な歴史イベントの発生年を聞くような問には、クイズのチャンピオンや Watson は難なく答えるだろうが、

「300年も続いた徳川幕府が大政奉還により、その権力を放棄することになった理由を述べよ」とか「大政奉還における徳川幕府と朝廷の関係を説明せよ」という問題には、現在の Watson でも答える能力は、まだないはず。

「現在の」と書いたのは、Joeperdy! に勝利した後、Watson は、さまざまに改善、拡張され、商品となっているから。その改善には、たとえば医療・ヘルスケアの専門的知識の獲得などがあることが[IBMの宣伝資料](http://www-935.ibm.com/industries/jp/ja/healthcare/)からわかる。

また、クイズに問われる様な「雑学的知識」では、大学の歴史学の教授をしのぐ知識を持つであろうクイズ・チャンピオンは、

これらの問題に答えることができるはずだが、その答えは、近世日本史の教授の答えには及ばないだろう。

また、Watson に、数学の問題を聞いたら、非常に単純な問題でも、<span class="red">多分</span>、答えられない。（多分と書いたのは、簡単な問題の場合、特に、センター入試風の問題では、これができるようにするのは、技術的には、それほど難しくはない筈だから。）

知力には、さまざま種類がある。

Watson や、エキスパート・システムの持つ知力というのは、基本的には、これらのシステムがデータベースの中に持つ情報を組み合わせて、推論を行うことである。

要するには、推論マシン、[推論エンジン](https://ja.wikipedia.org/wiki/%E6%8E%A8%E8%AB%96%E3%82%A8%E3%83%B3%E3%82%B8%E3%83%B3)である。Mycin の推論エンジンは、実にシンプルなものであって、基本的には、いわゆる[三段論法](https://ja.wikipedia.org/wiki/%E4%B8%89%E6%AE%B5%E8%AB%96%E6%B3%95)の一番代表的なケースである

AならばB

BならばC

よって

AならばC

を自動的にやっているのだと思えばよい。参考： [Wikipedia Inference Engine （推論エンジン）の Architecture](https://en.wikipedia.org/wiki/Inference_engine#Architecture).

三段論法のようなものを形式論理学というが、そこにおける形式的推論は、意味を理解しないでもできる機械的なもの。

だから、それを使って機械であるコンピュータに推論をさせようということになったのがエキスパート・システム。

しかし、人間は、多くの場合、形式的推論の前に、あるいは、他に、直観とか経験による判断を使う。

Deep Learning などのニューラルネットワークによるAIでは、それに類するものができるのだが、エキスパートシステムにはそれがなかった。

また、Watson では、それを取り込もうとしている兆候が見えるのだが、おそらくは、まだ中心的役割は果たしていない。

### 違い２　巨大なデータとその獲得方法の存在

第3次ブーム以前から、AIを研究しているベテランのAI研究者の間では、第3次ブームと第2次ブームのAIには、

差がなく、特に新しい技術が生まれたのではなくて、単に「ビッグデータ」あるいは巨大データが得られるようになっただけだ、というシニカルな見方をする人が結構多い。

たとえば、Watson の開発に従事した日本人として有名な、[日本IBMの武田浩一さんも、同じような意見](https://enterprisezine.jp/iti/detail/6436)を言っている。

*   ただし、これは第3次ブームにシニカルなのではなくて、巨大なデータの存在が本質的だったのだという意見。

これは、AIを技術面だけからみれば、一理ある意見なのだが、AIと社会との関係を考えるときには、「巨大データ」の存在と、

それを活用できるようにした、自然言語のテキストから情報を得る技術が開発されたことが大きい。

そして、もうひとつ、エキスパートシステムでは、存在はしたが、まだ、弱かった「統計的な思考法」とでもいうべき、

「少数のエキスパートが持つ厳密だけが役に立つのではなくて、むしろ、曖昧さを含む大量の社会的知識の方が、むしろ大きな力になる」

という、WEBの時代が生み出した、知識に関するパラダイム・チェンジが大きい。

Watson の背景哲学が集合知的であるということは、はっきり語られることはないようだが、その情報の収集方法や推論方式（いくつもの推論アルゴリズムを同時に走らせて、出てきた結果の中から正しそうなものを選ぶという方式）などを見ると、Watson は、明らかに、この哲学をもとに作られている。

そして、そのパラダイム・チェンジとは、集合的知識というもの。

ということで、次に、これを説明。

### 集合的知識 Collective Knowledge

[集合的知識](https://ja.wikipedia.org/wiki/集団的知性)とは？（この Wikipedia 記事では、集合的知性になっているが、知識の方がよい！）

以下では、集合知と書く。

[英語版 Wikipedia では。](https://en.wikipedia.org/wiki/Collective_intelligence)

簡単に説明すると、

*   集合知は、ある集団が、集団としてもつ有機的に結合された知識という意味で言われることが多いが、ここでは、一人のひとの知識、あるいは、非常に少数の人たち共通して持つ知識（たとえば、専門家の知識）と異なり、不正確な知識を持つ人たちも含む、大きな人間集団の知識の総体としてえられる知識のことだと思うことにする。
    *   例えば、選挙結果。
    *   Wikipediaの知識。
    *   Google の検索の Page Rank や Facebook のいいね、Amazon の review、Twitter のフォロワーなど。
    *   Linux などのソフトウェアの[オープン・ソース開発](https://ja.wikipedia.org/wiki/%E3%82%AA%E3%83%BC%E3%83%97%E3%83%B3%E3%82%BD%E3%83%BC%E3%82%B9%E3%82%BD%E3%83%95%E3%83%88%E3%82%A6%E3%82%A7%E3%82%A2)。
    *   [Iowa Electronic Markets](https://en.wikipedia.org/wiki/Iowa_Electronic_Markets)　<a name="2016.11.09">2016.11.09ここまで</a>
*   我々は、知識の正確性というのは、統計的なものではなく、正確な知識と推論で得られると思いがちである。
*   この信念が、専門家に大きな価値を与えている。
*   しかし、コンピュータの登場の後、多数の意見をリアルタイムで統計的に処理できるようになってから、特にWEBの登場後、「烏合の衆」と考えられ勝ちな素人の集団が、集団の要素の数が十分多い場合、専門家なみ、場合によっては、専門家より正確な知識を持つことが経験的に知られるようになった。
*   たとえば、Google 検索で、キーワードに関係が深いと思われるページを探すのには、Page Rank という指数が使われている（この Page は、頁の意味ではなくて、Google の創業者 L. Page の名前から来ている）。
*   この説明にあるように、たくさんのリンクがあることを、そのページが肯定的に注目されていることとみなして、注目度が高いもののランクを上にする。
*   現在では、それだけでなく、さまざまな方法が使われていて、Google 検索が、なぜ、的確な答えを出してくるか、その仕組みは管理している技術者でさえ説明できないような複雑なものなっているといわれている。
*   しかし、今でも、検索の最大の要素は、この Page Rank であると言われている。そして、Google が大企業に育った、理由の非常に大きな理由が、この Page Rank により、他の検索エンジンに比較して、はるかに適切な検索結果をユーザに提示できたこと。つまり、ユーザが期待するような結果を出力することが、他の検索エンジンより大幅に多かった。
*   考えてみると、これは不思議なこと。リンクには、例えば、否定的なものもあるだろう。あるWEBページにリンクして、「このページの内容は、嘘だらけだ！」と書くこともできる。
*   しかし、リンクする人とリンクの数が十分に大きい場合、この様な例は、例外として、肯定的なリンクの数に打ち消されてしまい、比較的正確な情報が得られるというのが、経験的に知られた「現実」だった。
*   この様なものが「集合知」の力。つまり、統計的な真理、多くの文書や、多くの筆者が語るものは、正しい可能性が高いということ。
*   ちなみに、質問票への回答の中で、例として挙げた[Reliability of Wikipedia](https://en.wikipedia.org/wiki/Reliability_of_Wikipedia)には、Wikipedia における集合知の問題点も論じられているように、集合知は絶対的なものではない。
*   しかし、おそらくは、これが最大のポイントなのだが、実は、個々の辞書、辞典、専門書や専門家の意見にも、多くの間違いがある。一般に人は気づかないし、出版社は、専門家が注意しても、重要な項目でなければ、分かっていても無視したりする（実際に、岩波広辞苑で林は経験した）。
*   もちろん、謙虚に、自分の書いたもの、自分の知識を振り返れば、そこには多くの間違いが見つかる。
*   これらのことを認識すれば、集合知の精度が思ったより高いということは、不思議なことではなくなる。

**<font color="#ff0000">もう一つの条件：</font>**　11月16日の質問票に、２ちゃんねるを集合知の例として、しかし、それは正確な知識の方向には進まないということを指摘しているものがあった。これは、面白い論点。2ちゃんねるの板にも、電気製品の評価など、非常に役立つ情報がある板がある。林が経験したのは、シーリング・ファン（天井につける扇風機）の板。研究室につけるシーリング・ファンを撰ぶ時、２ちゃんねるのシーリング・ファンの板に大変助けられた。  
　この板が、普通の２ちゃんねるの板と違っているのは、書き込みをしているユーザが皆、正しい情報を得たい、また、そのために自分も正しい情報を与えたいという態度で臨んでいたこと。おそらく、集合知というとき、この「参加の意識」を追加する必要があるのだろう。これは、 Wikipedia を成立させている条件でもある。また、これは「ユーザが<font color="#ff0000">皆</font>、正しい情報を得たい、また、そのために自分も正しい情報を与えたいという態度で臨でいる」ではなくて、「ユーザの<font color="#ff0000">圧倒的多数</font>が、正しい情報を得たい、また、そのために自分も正しい情報を与えたいという態度で臨でいる」という条件の場合も、統計的には、これは機能する。正しい情報を与えたいと思って行動するユーザーも、意図せず、間違えることが多いだろう。圧倒的多数に「参加意識」があり、訂正のインセンティブが働いている場合、皆が参加していることと、大多数が参加していることに、大きな差はないのだろう。

## IBM Watson と1980年代のエキスパート・システムの違い（再論）

さて、以上の集合知の話をもとに、IBM Watson と、1980年代のエキスパート・システムの違いを考えてみる。

上で、すでに引用したWatson project に参加された[日本IBMの武田浩一さんのインタビュー記事](https://enterprisezine.jp/iti/detail/6436)の内容を少し引用。

**太字は林の解説。**

*   **1980年代の第5世代コンピュータなどは、**IBMが現在推し進めているコグニティブ・コンピューティングとは何が違うのか？日本IBM 東京基礎研究所 技術理事の武田浩一氏に話を訊いた。
    *   **コグニティブ・コンピューティングとは、IBMが Watson などの同社の「AI」を呼ぶときの名称。**
    *   **IBMは意識的に、人工知能という名称を避けている。**
*   「（非手続き型プログラミング言語の）Prolog中心の推論エンジンを作ろうというものでした」と言うのは、日本IBM 東京基礎研究所 技術理事の武田浩一氏だ。この第5世代コンピュータは、現在IBMが強力に推し進めているコグニティブ・コンピューティングと考え方は似ている。
*   とはいえ、今と大きく違うのは当時はデータがなかったことだ。もちろん、Webも存在しなかければ、電子化されたWikipediaのようなものもない。「<span class="red">データがなかったことは、ものすごいハンディだったと思います」と武田氏</span>。
*   2011年、米国クイズ番組「ジョパディ!（Jeopardy!）」で人間のクイズ王に勝利したIBMのWatson。クイズの問いに対する答えを導き出すために、今ではWikipediaなどの電子化されたさまざまな情報がある。なので、クイズのような広範な質問に対し答えを見つけるための情報源に困ることはない。逆に言えば、そういった電子化されたあるいは電子化できる情報があるからこそ、Watsonのような自然言語で考えるコンピュータが実現できたとも言える。
*   たとえば、翻訳をしたりクイズに答えたりという場合には曖昧性が重要となります。より正解らしいものを提示する。そのための候補は、1つとは限りません」（武田氏）
*   ありそうな答えを出すことが、当時のAIの推論エンジンではできなかった。今のコグニティブ・コンピューティングでは、統計的な考え方なども取り込んで「たくさん正解が出るように集合的な最適化をしています」とのことだ。
    *   **ここは、第5世代コンピュータについては正しいが、エキスパート・システムについては、少し不正確な説明。**
    *   **エキスパート・システムは、答えが一つでも、それに確実度を表す数字がついていた。**
    *   **だから、単純に真か偽かで割り切ったのではなかった。**
    *   **これが現代の確実でない知識を表す技法の芽となったという意見もある。**
*   **記事では、この間に、IBMのチェスシステム、Deep　Blue で統計的方法が取り入れられ、第5世代コンピュータのころには、希薄だった、統計的で不確実な計算の有用性が認識されたという説明がある。これは実は、上に説明した集合知革命と、深い根において関係があるのだが、それは、今回は省略。かなり哲学的ともいえるものなので。ただ、根本的なパラダイム・チェンジなので、すごく重要なこと。**
*   Watsonの世界は、言い換えればコンピュータに多様性を取り入れるようなものだ。質問の答えは唯一ではなく、少量多品種を容認する世界でもある。そのための情報のインプットをプログラマーが考え手作業でいちちやることは現実的ではない。そこで注目を集めているのが、機械学習というわけだ。
*   「<span class="red">Watsonはデータがあったからうまくいった部分はあるでしょう。</span>データが増えていけば、精度は上がっていきます。今は、データが増えすぎて処理が頭打ちになることはありません。どんどん成長しており、もちろんそれに合わせハードウェアの成長もさらに進むことになる。まだまだ要求は衰えることはないでしょう」（武田氏）

林の、この講義は、経済産業研究所の人工知能の社会影響の研究プロジェクトを背景にしていることは、すでに説明した。

実は、その一環で、武田さんに、林のチームがインタビューを行った。林は病気で参加できなかったが、その記録を聴くと、最初、辞典からのデータ収集に期待をかけたが、実は、役立ったのは、Wikipedia だったという発言がある。

これが、おそらく、Watson　が成功するための条件であった<span class="red">データの存在</span>のことだったと思われる。つまり、上の記事の、2か所の赤文字の発言。

### テキストマイニングと unstructued data

Joeperdy! は雑学知識を問うクイズ番組なので、それに勝つためには、雑学データが必要。

そして、Joeperdy! で必要とさえる雑学データとは、たとえば、グラフで表されるような統計情報のようなものではない。

それは、テキスト・マイニングというもので、得られるデータで、その生データを用意したのが、先に説明した集合知革命の一つとして生まれた、Wikipedia という、巨大な、unstructued data　base 非構造的データベースであった。

そして、武田さんが言われているように、それが Watson 成功の一つのカギだったが、実は、もう一つのカギが必要だった。それは、上の記事の引用では、省略した、次の様な部分：

*   新たに注目を集めた自然言語処理の世界 　
    *   AIの空白期間を超え、2000年以降になり新たに注目され始めたのが人間が発する質問に対し答えるような、自然言語処理と呼ばれる世界だった。これは、IBMであれば後のWatsonの世界へとつながる。

次回、このテキストマイニングと unstructured data と自然言語処理の話。

簡単に言えば、stuructured data とは、Kulineの出力の様に、データが、その意味において、所定の場所に置かれているようなもの。たとえば、1行目の一番目の欄が本の番号で、3番目の欄が所蔵館などとわかる。

![](aSetOfBooks.PNG)

そして、同じ 0001 の蔵書の unstructured data とは、

No. 0001 の蔵書は、数理研究図書館に所蔵されていて、その配置場所は第一書庫、そして、その請求番号と資料IDは、それぞれ、HAY||10||1 と8788629213 である。

という文章のような data。

### サイバー空間の外と内にある膨大な unstructured data

世界には、膨大な数の unstructured data や、semi-structured data があふれている。  

コンピュータの普及以前は、多くのデータは、unstructured か、半ば structured だった。  

stuructured と言えたのは、OPAC 登場以前に、本を検索するために使われていた図書館カードのようなもの、帳簿などに限定されていた。  

図書館のカード (既に存在しないブログ http://toyohiro.at.webry.info/200812/article_7.html より)  

![](image.jpg)  

その構造（の一部）の説明  

![](image2.jpg)  

ちなみに、カードの棚と、その中身  

![](image1.jpg)  

![](image4.jpg)  
![](image3.jpg)  

この様にカードの情報は、Kulineが出力するものと同じような構造をもっていた。  

しかし、それを認識できるのは、人間でしかなかったので、例えば、京大文学部図書館で、こういうカードから、 Kuline に移行する際には、大きな予算をつけて、人を何にも雇い、カードの情報を、コンピュータのデータベースに移した。  

これを遡及入力という。今、文学の新館（本館？）の１Fの西の出口の北に、ラウンジがあるが、以前は、あの部屋は遡及入力作業のための部屋だった。  

京大の遡及入力について：<span style="font-size:14.0pt;font-family:&quot;ＭＳ Ｐゴシック&quot;;mso-ascii-font-family:
Calibri;mso-fareast-font-family:&quot;ＭＳ Ｐゴシック&quot;;mso-bidi-font-family:+mn-cs;
mso-ascii-theme-font:minor-latin;mso-fareast-theme-font:minor-fareast;
mso-bidi-theme-font:minor-bidi;color:black;mso-color-index:1;mso-font-kerning:
12.0pt;language:ja;text-combine:letters;mso-style-textfill-type:solid;
mso-style-textfill-fill-themecolor:text1;mso-style-textfill-fill-color:black;
mso-style-textfill-fill-alpha:100.0%">[静脩](http://hdl.handle.net/2433/49191)</span><span style="font-size:14.0pt;font-family:Calibri;mso-ascii-font-family:Calibri;
mso-fareast-font-family:&quot;ＭＳ Ｐゴシック&quot;;mso-bidi-font-family:+mn-cs;mso-ascii-theme-font:
minor-latin;mso-fareast-theme-font:minor-fareast;mso-bidi-theme-font:minor-bidi;
color:black;mso-color-index:1;mso-font-kerning:12.0pt;language:en-US;
text-combine:letters;mso-style-textfill-type:solid;mso-style-textfill-fill-themecolor:
text1;mso-style-textfill-fill-color:black;mso-style-textfill-fill-alpha:100.0%">[(Nov. 2007)](http://hdl.handle.net/2433/49191)</span><span style="font-size:14.0pt;font-family:&quot;ＭＳ Ｐゴシック&quot;;mso-ascii-font-family:Calibri;
mso-fareast-font-family:&quot;ＭＳ Ｐゴシック&quot;;mso-bidi-font-family:+mn-cs;mso-ascii-theme-font:
minor-latin;mso-fareast-theme-font:minor-fareast;mso-bidi-theme-font:minor-bidi;
color:black;mso-color-index:1;mso-font-kerning:12.0pt;language:ja;text-combine:
letters;mso-style-textfill-type:solid;mso-style-textfill-fill-themecolor:text1;
mso-style-textfill-fill-color:black;mso-style-textfill-fill-alpha:100.0%">[の記事](http://hdl.handle.net/2433/49191)</span>  

いずれにせよ、この遡及入力ということは**<font color="#ff0000">サイバー空間</font>**（WEBとか、PCのHDDとか、コンピュータの世界）の外にある 、ほぼ stuructured なデータをcyber空間の stuructured data にすること。  

そして、それは、論理的には structured でありながら、サイバー空間の外にあり人間にしか読めないために、膨大な人件費をかけて、Kuline 用の、stuructured data に変換する必要があった。（もちろん、ここに[OCR](https://ja.wikipedia.org/wiki/%E5%85%89%E5%AD%A6%E6%96%87%E5%AD%97%E8%AA%8D%E8%AD%98)とか人工知能を使うという方法もある。実際、九大では、それを一部やったらしい。）  

しかし、サイバー空間の外にある、最大の unstructured data は、実は、書籍、雑誌、書類、手書き文書など。  

これのものが、[Internet Library](https://archive.org/index.php) や [Google books](https://ja.wikipedia.org/wiki/Google_%E3%83%96%E3%83%83%E3%82%AF%E3%82%B9) により、画像、そして、それをOCRにより、コンピュータ上のテキストとして、サイバー空間上に「遡及入力」のように、移転されつつある。これが現状。  

しかし、これは、サイバー空間上にあるものの、unstructured data であることに注意。

#### 四種類のデータ

以上の、図書カードとInternet Library, Google Books の説明からすると、サイバー空間の内と外、stuructued か unstuructured かで、次の4種類のデータがあることになる。

1.  サイバー空間内＆structured: Kuline の様なOPACのデータなど
2.  サイバー空間内＆unstructured: Google Books などの電子化された書籍、PDFなど検索可能な形でWEB上に公開されている様々な書類、論文、書籍など
3.  サイバー空間外＆structured: 図書カードのデータなど
4.  サイバー空間外＆unstructured: 紙の上の書籍、書類など

この内の１では、「著者が夏目漱石、出版社が岩波書店の書籍」という風な検索ができる。また、２では、例えば、ある英語の単語を、1700年から2000年の間に出版された英語の本で、Google Books にあるものすべてから検索する、ということができる。  

この様に、サイバー空間の内のデータを、そのデータが持っている情報を、高速かつ大量に検索できるという、サイバー空間外のデータでは想像することも難しかったようなことが容易にできる。  

これが如何にすごいことかは、「著者が夏目漱石、出版社が岩波書店の書籍」を京大の総ての図書館で、上の画像のような図書カードで調べ尽くすという作業をイメージすればわかる。今では、それが一瞬で何の苦労もなくできてしまう。  

しかし、こういう OPAC データベースも、Google Books も、歴史についての質問、たとえば、It is the country of Asia which defeated Russia. というような Joerpady! の問題に答えることはできない。  

これに対して、上の4種類のデータの内、２の種類のデータを使って、こういうことをできるようにしたのが、IBM Watson。

### サイバー空間の unstructured data と IBM の Cognitive Computing

IBMは Watson をAIと呼ばず、Cognitive Computing と呼ぶ。  

そして、Watoson の Cognitive Computing の意味は、[この様に](http://www.ibm.com/cognitive/jp-ja/outthink/)説明されている：<span style="color: rgb(50, 50, 50); font-family: HelvRegularIBM, &quot;Helvetica Neue&quot;, Meiryo, メイリオ, sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); display: inline !important; float: none;">メールやソーシャル・メディア、動画、画像などの膨大な<font color="#ff0000">非構造化データをも理解</font>し、論理的に推論し、継続的に学習することができるシステムを活用。それを支えるプラットフォームがIBM Watson。<span class="Apple-converted-space"> </span>Watsonによって、製品、プロセス、ビジネスはコグニティブ（自ら思考できるよう）になり、私たちは、より多くの情報に基づいた確実性の高い意思決定ができるようになります。</span>  
 <span style="color: rgb(50, 50, 50); font-family: HelvRegularIBM, &quot;Helvetica Neue&quot;, Meiryo, メイリオ, sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); display: inline !important; float: none;">OPACでの検索の様に構造化されたものを理解するのは当然だから、cognitive computing のポイントは非構造化データ、つまり、unstructured data を、Watson　が利用しているということ。  

OPACとのアナロジーで考え、日本IBMの武田さんの発言「Watsonはデータがあったからうまくいった部分はあるでしょう」と、林たちのインタビューの際のWikipedia が役立ったという発言を考えれば、IBM Watson、あるいは、IBM Cognitive Computing とは、</span>次の様なものだと言えるだろう：

> **サイバー空間に存在する、膨大な structured data と、集合知革命によりサイバー空間の上に出現した Wikipedia の記事の様な膨大な unstructured dataが持つ情報を、1980年代に比較して遥かに進歩したテキストマイニングの技術を使うことにより、吸い上げ。そして、その情報をもとに、これも1980年代より遥かに進化した機械による推論を使うことにより、ユーザの自然言語による問いに答える様になった学習するエキスパート・システム。**

おそらく、最大のポイントは、武田さんが言うWikipedia の様な「データの存在」と、1980年代に比べて遥かに進化した[テキストマイニング](https://ja.wikipedia.org/wiki/%E3%83%86%E3%82%AD%E3%82%B9%E3%83%88%E3%83%9E%E3%82%A4%E3%83%8B%E3%83%B3%E3%82%B0)技術。このテキストマイニング技術というのは、自然言語やWEBページの様な、semi-structured なテキストから、有用なデータを抽出する技術で、[データマインイング](https://ja.wikipedia.org/wiki/%E3%83%87%E3%83%BC%E3%82%BF%E3%83%9E%E3%82%A4%E3%83%8B%E3%83%B3%E3%82%B0)技術の一種とされるもの。

現在のAIは、過去のAIと大して変わらず、ビッグデータの存在だけが違うというように言う専門家は多い。これは、人工知能を、「人間の様に考える人工物」と定義すれば、おそらく正しいのだろう。  

しかし、そうでなくて、「1910年1月にマハトマ・ガンジーは、彼の国の首都にいた。その首都は？」という問題に、自分で検索したり、辞典を引いたりして答えを見つけるのではなく、この文章で質問すれば答えてくれるようなシステムを実現できるようになった、ということは、実用性を考えれば、大変な進歩といえる。（ただし、これに本当にWatson が答えられるどうかはやってみたのではない。（＾＾；）しかし、答えられて全く可笑しくない質問。）  

実は、第5世代コンピュータ・プロジェクトでも、日本語をコンピュータに理解させるというサブプロジェクトが行われていた。その責任者は、当時、林とも仲が良かった向井国昭さんという方だった。  

ある日、林が週間朝日を読んでいたら、その向井さんが小学校の国語の教科書の文章をコンピュータに理解させることに成功したという記事が掲載されていた。ところが、後に向井さんに合ったときに、お話を聞いたら、あれは全くの嘘を書かれてしまって、大変迷惑しています、小学校の教科書の理解など、まだ全然できません、とのことだった。  

それを考えれば、今の Watson は、小学校の教科書どころか、Wikipedia からも情報を吸い上げる能力をもつ（ただし、これが向井さんが行おうとしていた、コンピュータに文章を理解させることであるかどうかは別の話）。しかも、それから推論をして、様々な答を出す能力を持つ。  

この推論についていえば、Watson では、推論の部分でも、幾つかの推論を方式で推論をさせて、確実性が高い結論を採るという方法が採用されているらしい。そういう、「蓋然性を持つ推論」の技術は、実は、第5世代のころにはなく、その後に、ベイジアン・ネットワークなどの理論ができて、さらに実用的になったもの。  

つまり、1980年代のエキスパートシステム比べて、Watson は、少なくとも、次の三つの進化を遂げている：

1.  テキストマイニングなどの unstructured data の解析・利用技術の進化
2.  Wikipedia あるいは、電子化された学術誌などの、巨大で有用な unstructured data の出現。
3.  ２から１により抽出されて蓄えられた巨大なデータから答を推論する、蓋然性を含む推論の技術の進化

つまり、Watson の成功を単純に、ビッグ・データの出現と見なすことはできない。とくに、誤解が生じるのは、ビッグデータという言葉。  

ビッグ・データという言葉が使われれる場合、通常は、Wikipedia のようなテキストを意味するのではなく、センサーなどで得られる構造化されたデータをいう事が多い。たとえば、スマホの位置情報など。これと Watson が使っていたデータはかなり違う事に注意。
# 人工知能と社会メモ　2016.11.30

### 質問票への回答のための資料

*   [ネイト・シルバー](https://ja.wikipedia.org/wiki/%E3%83%8D%E3%82%A4%E3%83%88%E3%83%BB%E3%82%B7%E3%83%AB%E3%83%90%E3%83%BC)

### 前回までの資料(まとめと修正を行ったもの）

## AIのタイプ

一括りに、AIというが、それには多くの異なる種類がある。その代表的なものとして、

*   deep learinig などの機械学習
*   1980年代のエキスパートシステムと第5世代コンピュータ
*   エキスパートシステムの進化形としての、IBM Watson

の話をしてきた。

今回は、IBM Watson の続き、及びに、最近、東大合格を諦めたという報道が流れた「東ロボくん」プロジェクトについて。

東ロボくんは、古参のAI研究者からは、非常に評判が悪い。しかし、これは「人工知能と社会」という視点からは、大変に興味深いプロジェクトである。

このプロジェクトを「社会的存在」として分析していくと、多くの日本人が、「知」やAIというものをどうとらえているのかが、炙りだされる。

その問題は、単にAIというITの問題ではなく、「日本社会と知」という大きな問題に繋がる。

## IBM Watson

アメリカの人気クイズ番組で、チャンピオンを破った、IBMのAIである、Watsonについての解説。

## IBM Watson と1980年代のエキスパート・システムの違い（再論）

さて、以上の集合知の話をもとに、IBM Watson と、1980年代のエキスパート・システムの違いを考えてみる。

上で、すでに引用したWatson project に参加された[日本IBMの武田浩一さんのインタビュー記事](https://enterprisezine.jp/iti/detail/6436)の内容を少し引用。

**太字は林の解説。**

*   **1980年代の第5世代コンピュータなどは、**IBMが現在推し進めているコグニティブ・コンピューティングとは何が違うのか？日本IBM 東京基礎研究所 技術理事の武田浩一氏に話を訊いた。
    *   **コグニティブ・コンピューティングとは、IBMが Watson などの同社の「AI」を呼ぶときの名称。**
    *   **IBMは意識的に、人工知能という名称を避けている。**
*   「（非手続き型プログラミング言語の）Prolog中心の推論エンジンを作ろうというものでした」と言うのは、日本IBM 東京基礎研究所 技術理事の武田浩一氏だ。この第5世代コンピュータは、現在IBMが強力に推し進めているコグニティブ・コンピューティングと考え方は似ている。
*   とはいえ、今と大きく違うのは当時はデータがなかったことだ。もちろん、Webも存在しなかければ、電子化されたWikipediaのようなものもない。「<span class="red">データがなかったことは、ものすごいハンディだったと思います」と武田氏</span>。
*   2011年、米国クイズ番組「ジョパディ!（Jeopardy!）」で人間のクイズ王に勝利したIBMのWatson。クイズの問いに対する答えを導き出すために、今ではWikipediaなどの電子化されたさまざまな情報がある。なので、クイズのような広範な質問に対し答えを見つけるための情報源に困ることはない。逆に言えば、そういった電子化されたあるいは電子化できる情報があるからこそ、Watsonのような自然言語で考えるコンピュータが実現できたとも言える。
*   たとえば、翻訳をしたりクイズに答えたりという場合には曖昧性が重要となります。より正解らしいものを提示する。そのための候補は、1つとは限りません」（武田氏）
*   ありそうな答えを出すことが、当時のAIの推論エンジンではできなかった。今のコグニティブ・コンピューティングでは、統計的な考え方なども取り込んで「たくさん正解が出るように集合的な最適化をしています」とのことだ。
    *   **ここは、第5世代コンピュータについては正しいが、エキスパート・システムについては、少し不正確な説明。**
    *   **エキスパート・システムは、答えが一つでも、それに確実度を表す数字がついていた。**
    *   **だから、単純に真か偽かで割り切ったのではなかった。**
    *   **これが現代の確実でない知識を表す技法の芽となったという意見もある。**
*   **記事では、この間に、IBMのチェスシステム、Deep　Blue で統計的方法が取り入れられ、第5世代コンピュータのころには、希薄だった、統計的で不確実な計算の有用性が認識されたという説明がある。これは実は、上に説明した集合知革命と、深い根において関係があるのだが、それは、今回は省略。かなり哲学的ともいえるものなので。ただ、根本的なパラダイム・チェンジなので、すごく重要なこと。**
*   Watsonの世界は、言い換えればコンピュータに多様性を取り入れるようなものだ。質問の答えは唯一ではなく、少量多品種を容認する世界でもある。そのための情報のインプットをプログラマーが考え手作業でいちちやることは現実的ではない。そこで注目を集めているのが、機械学習というわけだ。
*   「<span class="red">Watsonはデータがあったからうまくいった部分はあるでしょう。</span>データが増えていけば、精度は上がっていきます。今は、データが増えすぎて処理が頭打ちになることはありません。どんどん成長しており、もちろんそれに合わせハードウェアの成長もさらに進むことになる。まだまだ要求は衰えることはないでしょう」（武田氏）

林の、この講義は、経済産業研究所の人工知能の社会影響の研究プロジェクトを背景にしていることは、すでに説明した。

実は、その一環で、武田さんに、林のチームがインタビューを行った。林は病気で参加できなかったが、その記録を聴くと、最初、辞典からのデータ収集に期待をかけたが、実は、役立ったのは、Wikipedia だったという発言がある。

これが、おそらく、Watson　が成功するための条件であった<span class="red">データの存在</span>のことだったと思われる。つまり、上の記事の、2か所の赤文字の発言。

### テキストマイニングと unstructued data

Joeperdy! は雑学知識を問うクイズ番組なので、それに勝つためには、雑学データが必要。

そして、Joeperdy! で必要とさえる雑学データとは、たとえば、グラフで表されるような統計情報のようなものではない。

それは、テキスト・マイニングというもので、得られるデータで、その生データを用意したのが、先に説明した集合知革命の一つとして生まれた、Wikipedia という、巨大な、unstructued data　base 非構造的データベースであった。

そして、武田さんが言われているように、それが Watson 成功の一つのカギだったが、実は、もう一つのカギが必要だった。それは、上の記事の引用では、省略した、次の様な部分：

*   新たに注目を集めた自然言語処理の世界 　
    *   AIの空白期間を超え、2000年以降になり新たに注目され始めたのが人間が発する質問に対し答えるような、自然言語処理と呼ばれる世界だった。これは、IBMであれば後のWatsonの世界へとつながる。

次回、このテキストマイニングと unstructured data と自然言語処理の話。

簡単に言えば、stuructured data とは、Kulineの出力の様に、データが、その意味において、所定の場所に置かれているようなもの。たとえば、1行目の一番目の欄が本の番号で、3番目の欄が所蔵館などとわかる。

![](aSetOfBooks.PNG)

そして、同じ 0001 の蔵書の unstructured data とは、

No. 0001 の蔵書は、数理研究図書館に所蔵されていて、その配置場所は第一書庫、そして、その請求番号と資料IDは、それぞれ、HAY||10||1 と8788629213 である。

という文章のような data。

### サイバー空間の外と内にある膨大な unstructured data

世界には、膨大な数の unstructured data や、semi-structured data があふれている。  

コンピュータの普及以前は、多くのデータは、unstructured か、半ば structured だった。  

stuructured と言えたのは、OPAC 登場以前に、本を検索するために使われていた図書館カードのようなもの、帳簿などに限定されていた。  

図書館のカード (既に存在しないブログ http://toyohiro.at.webry.info/200812/article_7.html より)  

![](image.jpg)  

その構造（の一部）の説明  

![](image2.jpg)  

ちなみに、カードの棚と、その中身  

![](image1.jpg)  

![](image4.jpg)  
![](image3.jpg)  

この様にカードの情報は、Kulineが出力するものと同じような構造をもっていた。  

しかし、それを認識できるのは、人間でしかなかったので、例えば、京大文学部図書館で、こういうカードから、 Kuline に移行する際には、大きな予算をつけて、人を何にも雇い、カードの情報を、コンピュータのデータベースに移した。  

これを遡及入力という。今、文学の新館（本館？）の１Fの西の出口の北に、ラウンジがあるが、以前は、あの部屋は遡及入力作業のための部屋だった。  

京大の遡及入力について：<span style="font-size:14.0pt;font-family:&quot;ＭＳ Ｐゴシック&quot;;mso-ascii-font-family:
Calibri;mso-fareast-font-family:&quot;ＭＳ Ｐゴシック&quot;;mso-bidi-font-family:+mn-cs;
mso-ascii-theme-font:minor-latin;mso-fareast-theme-font:minor-fareast;
mso-bidi-theme-font:minor-bidi;color:black;mso-color-index:1;mso-font-kerning:
12.0pt;language:ja;text-combine:letters;mso-style-textfill-type:solid;
mso-style-textfill-fill-themecolor:text1;mso-style-textfill-fill-color:black;
mso-style-textfill-fill-alpha:100.0%">[静脩](http://hdl.handle.net/2433/49191)</span><span style="font-size:14.0pt;font-family:Calibri;mso-ascii-font-family:Calibri;
mso-fareast-font-family:&quot;ＭＳ Ｐゴシック&quot;;mso-bidi-font-family:+mn-cs;mso-ascii-theme-font:
minor-latin;mso-fareast-theme-font:minor-fareast;mso-bidi-theme-font:minor-bidi;
color:black;mso-color-index:1;mso-font-kerning:12.0pt;language:en-US;
text-combine:letters;mso-style-textfill-type:solid;mso-style-textfill-fill-themecolor:
text1;mso-style-textfill-fill-color:black;mso-style-textfill-fill-alpha:100.0%">[(Nov. 2007)](http://hdl.handle.net/2433/49191)</span><span style="font-size:14.0pt;font-family:&quot;ＭＳ Ｐゴシック&quot;;mso-ascii-font-family:Calibri;
mso-fareast-font-family:&quot;ＭＳ Ｐゴシック&quot;;mso-bidi-font-family:+mn-cs;mso-ascii-theme-font:
minor-latin;mso-fareast-theme-font:minor-fareast;mso-bidi-theme-font:minor-bidi;
color:black;mso-color-index:1;mso-font-kerning:12.0pt;language:ja;text-combine:
letters;mso-style-textfill-type:solid;mso-style-textfill-fill-themecolor:text1;
mso-style-textfill-fill-color:black;mso-style-textfill-fill-alpha:100.0%">[の記事](http://hdl.handle.net/2433/49191)</span>  

いずれにせよ、この遡及入力ということは**<font color="#ff0000">サイバー空間</font>**（WEBとか、PCのHDDとか、コンピュータの世界）の外にある 、ほぼ stuructured なデータをcyber空間の stuructured data にすること。  

そして、それは、論理的には structured でありながら、サイバー空間の外にあり人間にしか読めないために、膨大な人件費をかけて、Kuline 用の、stuructured data に変換する必要があった。（もちろん、ここに[OCR](https://ja.wikipedia.org/wiki/%E5%85%89%E5%AD%A6%E6%96%87%E5%AD%97%E8%AA%8D%E8%AD%98)とか人工知能を使うという方法もある。実際、九大では、それを一部やったらしい。）  

しかし、サイバー空間の外にある、最大の unstructured data は、実は、書籍、雑誌、書類、手書き文書など。  

これのものが、[Internet Library](https://archive.org/index.php) や [Google books](https://ja.wikipedia.org/wiki/Google_%E3%83%96%E3%83%83%E3%82%AF%E3%82%B9) により、画像、そして、それをOCRにより、コンピュータ上のテキストとして、サイバー空間上に「遡及入力」のように、移転されつつある。これが現状。  

しかし、これは、サイバー空間上にあるものの、unstructured data であることに注意。

#### 四種類のデータ

以上の、図書カードとInternet Library, Google Books の説明からすると、サイバー空間の内と外、stuructued か unstuructured かで、次の4種類のデータがあることになる。

1.  サイバー空間内＆structured: Kuline の様なOPACのデータなど
2.  サイバー空間内＆unstructured: Google Books などの電子化された書籍、PDFなど検索可能な形でWEB上に公開されている様々な書類、論文、書籍など
3.  サイバー空間外＆structured: 図書カードのデータなど
4.  サイバー空間外＆unstructured: 紙の上の書籍、書類など

この内の１では、「著者が夏目漱石、出版社が岩波書店の書籍」という風な検索ができる。また、２では、例えば、ある英語の単語を、1700年から2000年の間に出版された英語の本で、Google Books にあるものすべてから検索する、ということができる。  

この様に、サイバー空間の内のデータを、そのデータが持っている情報を、高速かつ大量に検索できるという、サイバー空間外のデータでは想像することも難しかったようなことが容易にできる。  

これが如何にすごいことかは、「著者が夏目漱石、出版社が岩波書店の書籍」を京大の総ての図書館で、上の画像のような図書カードで調べ尽くすという作業をイメージすればわかる。今では、それが一瞬で何の苦労もなくできてしまう。  

しかし、こういう OPAC データベースも、Google Books も、歴史についての質問、たとえば、It is the country of Asia which defeated Russia. というような Joerpady! の問題に答えることはできない。  

これに対して、上の4種類のデータの内、２の種類のデータを使って、こういうことをできるようにしたのが、IBM Watson。

### サイバー空間の unstructured data と IBM の Cognitive Computing

IBMは Watson をAIと呼ばず、Cognitive Computing と呼ぶ。  

そして、Watoson の Cognitive Computing の意味は、[この様に](http://www.ibm.com/cognitive/jp-ja/outthink/)説明されている：<span style="color: rgb(50, 50, 50); font-family: HelvRegularIBM, &quot;Helvetica Neue&quot;, Meiryo, メイリオ, sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); display: inline !important; float: none;">メールやソーシャル・メディア、動画、画像などの膨大な<font color="#ff0000">非構造化データをも理解</font>し、論理的に推論し、継続的に学習することができるシステムを活用。それを支えるプラットフォームがIBM Watson。<span class="Apple-converted-space"> </span>Watsonによって、製品、プロセス、ビジネスはコグニティブ（自ら思考できるよう）になり、私たちは、より多くの情報に基づいた確実性の高い意思決定ができるようになります。</span>  
 <span style="color: rgb(50, 50, 50); font-family: HelvRegularIBM, &quot;Helvetica Neue&quot;, Meiryo, メイリオ, sans-serif; font-size: 16px; font-style: normal; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); display: inline !important; float: none;">OPACでの検索の様に構造化されたものを理解するのは当然だから、cognitive computing のポイントは非構造化データ、つまり、unstructured data を、Watson　が利用しているということ。  

OPACとのアナロジーで考え、日本IBMの武田さんの発言「Watsonはデータがあったからうまくいった部分はあるでしょう」と、林たちのインタビューの際のWikipedia が役立ったという発言を考えれば、IBM Watson、あるいは、IBM Cognitive Computing とは、</span>次の様なものだと言えるだろう：

> **サイバー空間に存在する、膨大な structured data と、集合知革命によりサイバー空間の上に出現した Wikipedia の記事の様な膨大な unstructured dataが持つ情報を、1980年代に比較して遥かに進歩したテキストマイニングの技術を使うことにより、吸い上げ。そして、その情報をもとに、これも1980年代より遥かに進化した機械による推論を使うことにより、ユーザの自然言語による問いに答える様になった学習するエキスパート・システム。**

おそらく、最大のポイントは、武田さんが言うWikipedia の様な「データの存在」と、1980年代に比べて遥かに進化した[テキストマイニング](https://ja.wikipedia.org/wiki/%E3%83%86%E3%82%AD%E3%82%B9%E3%83%88%E3%83%9E%E3%82%A4%E3%83%8B%E3%83%B3%E3%82%B0)技術。このテキストマイニング技術というのは、自然言語や、WEBページの様な、semi-structured なテキストから、有用なデータを抽出する技術で、[データマインイング](https://ja.wikipedia.org/wiki/%E3%83%87%E3%83%BC%E3%82%BF%E3%83%9E%E3%82%A4%E3%83%8B%E3%83%B3%E3%82%B0)技術の一種とされるもの。

現在のAIは、過去のAIと大して変わらず、ビッグデータの存在だけが違うというように言う専門家は多い。これは、人工知能を、「人間の様に考える人工物」と定義すれば、おそらく正しいのだろう。  

しかし、そうでなくて、「1910年1月にマハトマ・ガンジーは、彼の国の首都にいた。その首都は？」という問題に、自分で検索したり、辞典を引いたりして答えを見つけるのではなく、この文章で質問すれば答えてくれるようなシステムを実現できるようになった、ということは、実用性を考えれば、大変な進歩といえる。（ただし、これに本当にWatson が答えられるどうかはやってみたのではない。（＾＾；）しかし、答えられて全く可笑しくない質問。）  

実は、第5世代コンピュータ・プロジェクトでも、日本語をコンピュータに理解させるというサブプロジェクトが行われていた。その責任者は、当時、林とも仲が良かった向井国昭さんという方だった。  

ある日、林が週間朝日を読んでいたら、その向井さんが小学校の国語の教科書の文章をコンピュータに理解させることに成功したという記事が掲載されていた。ところが、後に向井さんに合ったときに、お話を聞いたら、あれは全くの嘘を書かれてしまって、大変迷惑しています、小学校の教科書の理解など、まだ全然できません、とのことだった。  

それを考えれば、今の Watson は、小学校の教科書どころか、Wikipedia からも情報を吸い上げる能力をもつ（ただし、これが向井さんが行おうとしていた、コンピュータに文章を理解させることであるかどうかは別の話）。しかも、それから推論をして、様々な答を出す能力を持つ。  

この推論についていえば、Watson では、推論の部分でも、幾つかの推論を方式で推論をさせて、確実性が高い結論を採るという方法が採用されているらしい。そういう、「蓋然性を持つ推論」の技術は、実は、第5世代のころにはなく、その後に、ベイジアン・ネットワークなどの理論ができて、さらに実用的になったもの。  

つまり、1980年代のエキスパートシステム比べて、Watson は、少なくとも、次の三つの進化を遂げている：

1.  テキストマイニングなどの unstructured data の解析・利用技術の進化
2.  Wikipedia あるいは、電子化された学術誌などの、巨大で有用な unstructured data の出現。
3.  ２から１により抽出されて蓄えられた巨大なデータから答を推論する、蓋然性を含む推論の技術の進化

つまり、Watson の成功を単純に、ビッグ・データの出現と見なすことはできない。とくに、誤解が生じるのは、ビッグデータという言葉。  

ビッグ・データという言葉が使われれる場合、通常は、Wikipedia のようなテキストを意味するのではなく、センサーなどで得られる構造化されたデータをいう事が多い。たとえば、スマホの位置情報など。これと Watson が使っていたデータはかなり違う事に注意。

#### ここから今回の資料

### 東ロボくんプロジェクト

いままで説明してきた、deep learning, IBM Watson は、今回のAIブームの代表的事例だが、日本においては、これとは別に、2011年に始まった「[東ロボくん](https://ja.wikipedia.org/wiki/東ロボくん#cite_note-nhk20161108-2)」というプロジェクトがあり、それがAIブームの火付け役の一つだったといえる。

その具体的目標は、同プロジェクトのサイトによると、次の様である。

*   本プロジェクトの具体的なベンチマークとして、2016年度までに大学入試センター試験で高得点をマークすること、また2021年度に東京大学入試を突破することを目標に研究活動を進めています。

これは、NII　国立情報学研究所の内部プロジェクトで、先に説明した1980年代の第5世代コンピュータプロジェクトの様な国家プロジェクトと違い、極小規模なもの。

しかし、出身大学が強く意識されることが多く、また、大学入試というものに、肯定的ｊかつ否定的な強い文化的意味が付与されている日本社会では、その大学入試のヒエラルキーのトップにある東大の入試に、機械であるロボット（実際には、ソフトウェア群）が合格するというコンセプトは、多くのひとびとのプライドや劣等感を揺さぶるものであったため、大きくマスコミに取り上げられた。

つまり、ほとんどの人にとっては、東大の入試を「突破する」ということは、憧れの対象であると同時に、自分には無理だという劣等感の対象となっていると思われる。また、その故に「日本の大学入試は、実社会では役に立たない無駄な知力を計るもの」、あるいは、「実社会においては東大入試を突破したことは無意味」などと言われることが多い。これは、もちろん、No2だと思われている京大でも同じこと。

このプロジェクトの実態を知り、また、それに対する社会の反応を分析し、さらには、それらを元に、日本の大学入試制度というものの背景にある「日常的に見逃している前提」を分析すると、AIと社会の関係について、大変に面白いことが沢山わかる。

また、それだけでなくて、このプロジェクトに対する誤解を理解すると、色々な社会問題を考えるときに、陥りがちな誤解を避けられるようになる可能性が高い。AIと社会について考えるときには、特に、そういう誤解、思い込みを回避することが必要である。

そこで、このプロジェクトと、それに社会がどう反応したかという分析を、もう少し進んだときにやる予定でいたが（シラバスの(授業計画と内容)の６）、今月になって、Watsonの解説をしている所で、東ロボくんが東大入試突破を断念したというニュースが報道され、これに対する反応が、林が想像したより大きかった。それで、ちょうど、良いので、予定を繰り上げて、ここで、このプロジェクトの解説と、それに対する社会の反応の分析を行う。

実は、このプロジェクトの提案者で、リーダーである新井教授は林の知り合いで（ご本人のお話では、新井教授がまだ院生だったころから）、今も、いろいろな仕事でお会いしたり連絡を取り合うことがある。その関係で、個人的に伺った話からの情報も入っている。

#### 東ロボくんプロジェクトとは

2011年に、東京にある国立情報学研究所、以下、NIIで始まったプロジェクト。

リーダは、新井紀子教授。新井教授は、AIの専門家ではないが、NIIの部門の長であったためか、所員の業績評価のために、業績を見ているうちに、東大の入試をAIで解けるのではないかと思いつき、これを同研究所のAIの専門家に聞いてみたところ、可能という返事だったので、このプロジェクトを所長に提案したとのこと。

ご本人から直接聞いた話では、前年の2010年に出版した、[「コンピュータが仕事を奪う」という本](https://www.amazon.co.jp/%E3%82%B3%E3%83%B3%E3%83%94%E3%83%A5%E3%83%BC%E3%82%BF%E3%81%8C%E4%BB%95%E4%BA%8B%E3%82%92%E5%A5%AA%E3%81%86-%E6%96%B0%E4%BA%95-%E7%B4%80%E5%AD%90/dp/4532316707/ref=sr_1_1?ie=UTF8&qid=1480482877&sr=8-1&keywords=%E6%96%B0%E4%BA%95%E7%B4%80%E5%AD%90)に対する、社会の反応が、当初鈍かったので、それに対する危惧がプロジェクトを発想した背後にあるとのこと。それは、新井教授の日経産業新聞への、[この寄稿](http://www.nikkei.com/article/DGXKZO94675990S5A201C1X12000/)からもうかがえる。

この本は、多くの知的仕事が、コンピュータによって代替できるような時代が来ている、人間はコンピュータではできないような高度の知的能力を持たないといけないというような本。

実は、日本では、ほとんど知られていないが、アメリカでは、これより少し後に、機械、特にITにより人間が失業すると警告する本が、何冊も出版されてベストセラーになった。

*   [The Second Machine Age: Work, Progress, and Prosperity in a Time of Brilliant Technologies](https://www.amazon.com/Second-Machine-Age-Prosperity-Technologies/dp/0393239357/ref=pd_sim_14_1?_encoding=UTF8&psc=1&refRID=5TD0PQ1TACPAKBSSJJZT)
*   [Race Against the Machine: How the Digital Revolution is Accelerating Innovation, Driving Productivity, and Irreversibly Transforming Employment and the Economy](https://www.amazon.com/Race-Against-Machine-Accelerating-Productivity/dp/0984725113)

新井教授が、以前から行っていた数学啓蒙活動の延長上で書かれているために、これらの「警告の書」ほどには、警鐘としての強さはないが、専門家のみが共有していたと思われる「起こりそうな危機」を社会に向けて警告した良い本である。

日本社会で関心が強い、入試を機械が突破すれば、社会も、大きな問題に気づくだろう、という発想だったらしい。

林は、AIの専門家ではないが、2005年ころから、AIとはかなり違う目的ながら、機械学習と関連した論理学を作っていた関係から、友人で機械学習の専門家である、現情報学研究科長の山本教授に、機械学習の応用について、いろいろと教えていただいていたので、AIの持つ可能性と危険性について、ある程度理解できていた。それからすると、機械学習などの専門家の間では、かなり早い段階で、人間の知的労働をAIなどで replace できるという可能性が理解されていたのではないかと思う。

たとえば、WEBの黎明期、知りたい情報を現在の検索エンジンの様なもので見つけ出すことは容易ではなく、そのため、当時の Yahoo！のように、項目ごとに、関連ある情報、重要な情報を整理・分類して階層的にみせる[デパートなどの案内掲示板](https://www.google.co.jp/search?q=%E3%83%87%E3%83%91%E3%83%BC%E3%83%88%E3%80%80%E6%A1%88%E5%86%85%E6%9D%BF&biw=1920&bih=950&source=lnms&tbm=isch&sa=X&ved=0ahUKEwiwrvfYu9DQAhUDiLwKHRHLBngQ_AUIBigB)の様なものがあった。

Yahoo! では、これを多くの従業員を使ってやっていたが、Google が登場して、関連が深いものを自動で検索することが可能となり、こういうビジネスモデルはすたれ、Yahoo! も、Web検索は、Google 方式に切り替えている。

つまり、仕事としてWWWをブラウズしていた、多数の高度な知性をもつ Yahoo! の従業員たちの仕事が、Google の検索に置きかえられたわけで、この Google の検索方式も、ある意味ではAIと言ってよいようなものだった。

実際、その頃の、Google社の検索方式の中心は、創業者のひとりの Larry Page の名前にちなむ、[PageRank](https://ja.wikipedia.org/wiki/%E3%83%9A%E3%83%BC%E3%82%B8%E3%83%A9%E3%83%B3%E3%82%AF) という技術だったが、これは、リンクを「投資」のようにとらえると（関心が高いページにリンクすることを、そのページに投資したと考える）、集合知のところで説明した、[Iowa Electronic Markets](https://iem.uiowa.edu/iem/)の様な集合知の一種であることがわかる。つまり、これも、IBM Watson 的のような、WEBなどに含まれる大規模な人間の知を抽出する一種のAI、あるいは、Cognitive Computing 的なものなのである（ただし、当時の Google 検索は、unstructured dataの分析はしていない）。

Google　社が、かなり早い時期からAIに関心をもっていたのは、これらのことが背景にあると考えられる。Page は、すでに、 2000 年に、次の様に発言している　[Interview　Larry Page, October 28, 2000, London, England](http://bigdata-madesimple.com/12-famous-quotes-on-artificial-intelligence-by-google-founders/)

*   Artificial intelligence would be the ultimate version of Google. The ultimate search engine that would understand everything on the Web.

#### 東ロボくんプロジェクトは失敗したのか？

先日、京大の東京オフィスで、[AIと社会についての講演](http://www.shayashi.jp/20161115tokyoVer20161116.pdf)をしたことは、前回述べた。

この講義は、日本のAIの産業化・研究には、米国のそれに比べて問題があるという論点に基づくものだった。

そのため、講演後の質問で、東ロボくんが東大入学をあきらめたのも、そのような理由か、という質問が出た。

つまり、東ロボくんプロジェクトが失敗したとみなし、それの失敗の原因は、林がいう様な日本のAIの問題点と同じか、という質問であった。

他にも、東ロボくんプロジェクトが失敗したと理解している人が、林の文学部の同僚にもいた。

これについては、新井教授は、少なくとも昨年くらいからは、「東大には入れないと思う」と、講演などで言っていたし、今年の夏ごろに「予定どおり入れないという結論になりました」と個人的に話していた。

プロジェクトの目的は、一般向けには、「東大にAIを合格させる」と説明されていたが、専門家向けの解説などを見ると、現在のAIの限界を見ることがその目的だと書かれていたりする。

また、林が新井教授から聞いていた、背景的な目的であった、日本社会への警告という、ことについては、大学入試センター試験の模試で、それなりの成績を示している、現状でも十分警告になっている。

日本社会が、その警告を十分受け止めたかどうかという点では、危惧が残るが、その意味では、決して失敗したわけではない。

昨日の、[京都新聞のコラム「凡語」](http://www.kyoto-np.co.jp/info/bongo/20161129_2.html)では、「凡人である自分は、東ロボくんが東大をあきらめたということを知り、ほっとした」という意味のことが書かれていた。しかし、同時に、新聞記者も安穏としているわけにはいかないのだろうということも書かれている。これなどは、警告に成功したという例だろう。

ただ、果たして、どれだけ、マスコミや日本社会が、このプロジェクトの本当の意味を理解できたかどうかには、疑問が残る。

実は、これには、Watson の話のときに、歴史学の大学教授の知と、クイズ王の知、Watsonの知、が、それぞれ、違っていて、どれが優れていると簡単に比較できないと説明したが、それと同じ問題が潜んでいる。

それは「知の種類」と「知をいかにして測るか」という問題。

#### Excel で簡単に解ける京大文系の入試問題：京大は実は入試問題を解ける学生に来て欲しいのではない

これは、ずいぶん前に、林のブログに書いた話。

### [入学試験と人間とIT・AIの能力　その１](http://www.shayashi.jp/xoopsMain/html/modules/wordpress/index.php?p=289 "Permanent Link: 入学試験と人間とIT・AIの能力　その１")

### [入学試験と人間とIT・AIの能力　その2](http://www.shayashi.jp/xoopsMain/html/modules/wordpress/index.php?p=291 "Permanent Link: 入学試験と人間とIT・AIの能力　その2")

要するには、2014年3月に実施された京大の入試の文系の数学の問題４の（２）が、Excel を使うと苦も無く解けるということ。

京大は、そういう風にして問題を解く学生が欲しいわけではない。

上のブログに詳しく書いてあるように、入学試験では、いろいろな制約が数多く定められている。

たとえば、一定時間内に答えること、また、辞書などは使えないこと、WEB検索は許されないこと、などである。

そういうがちがちの条件下で、ある一定のパーフォーマンスを出すということは、Jeorpady! のようなクイズ番組に似ている。

そして、その「クイズ」に勝てば、受験生は入学を許されるのであるが、これは我々京大教員が、そういう「入学試験というクイズ」に強い人を育てたいから、入試問題を受験生に課すわけではない。

多くの京大文学部の教員は、現在の入試制度が、同じくらいのコストしかかけられないならば、最善の選抜方法だと思っていると思うが（林はそう思っている）、それは入試に強い人が欲しいからではない。

みなさんも、見聞きしたことがあるだろうが、高校では、開学以来の神童と言われながら、大学に入ると、まったく学業が振るわないひとがいる。

実は、高校までの勉強と、大学以上の本当の学問というものは、ずいぶん違う。京大の教員が学生として欲しいと思う人とは、その本物の学問ができる人である。

しかし、その本当の学問を高校生に課して選抜することは、もちろん不可能である。最善の方法は、アメリカ流に、ゆるく選抜して、実際の学問のパーフォーマンスが悪ければ、大学を去ってもらう、というのが一番良い。

しかし、これには、大掛かりな社会構造や社会の意識の改変が必要で、残念ながら日本では行うのは非常に難しい。

そのため、この能力が高い人は、今までの経験からして、多くの場合に、我々が欲しいと思う能力を持っている、というような能力を見る問題。それが入試問題である。

たとえば、相撲取りの場合、ある一定の身長以上でないと相撲取りの資格がないとされる。そのために、[頭のてっぺんにシリコンを入れて新弟子検査に合格し、後に有名な相撲取りになったという例](http://next.spotlight-media.jp/article/78881099083876694)がある。

この場合、相撲協会が欲しい人材は、背が高い人材ではなくて、良い相撲取りになれる人材のはずだが、それは採用してみて、長年たって、結果論としてわかるに過ぎないので、多くの場合には、小さい人は相撲には適さないというので落とすわけである。

実は、入試の場合も、これに非常によく似ている。

従って、東大の入試に、もし何らかの意味でAIが合格できても、それが直接、東大を出て、社会で活躍する、学問で活躍するような人がもつ知性と同様なAIができたということにはならないのである。

試験というものは、あくまで相関関係を見て、ある意味で「統計的」に人間の知力を見ているだけなのである。

また、東ロボくんが解いたのは、本当の東大の入試問題ではなくて、その模試である。ここにも、模試と本当の入試は違うという問題がある。

また、東ロボくんの実験を報告する論文をみると、明らかに、長い時間をかけて、問題を解かせていることがわかる。

したがって、中くらいの大学の偏差値を上回ったからと言って、そういう大学の卒業生たちがついている職業を、AIで置き換えることができる時代になったということではないのである。

ただし、一方で、そのような力があれば、実際に、人間の職を奪う可能性は、直接に分析をしていけば、見えてくるのである。

たとえば、上に引用したコラムで、京都新聞の記者が、AI記者の存在を知って、記者の仕事も危ないと理解したのは正しい判断である。

この様に、直接に具体的仕事を見て、AIがそれを置き換える可能性を考えなくてはいけない。

#### 東ロボくんは、本当にAIか？

次回に続く